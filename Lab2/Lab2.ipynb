{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "905619c4-58e5-4e3c-b8c7-ddbfe94a12e6",
   "metadata": {},
   "source": [
    "# A. Préparation de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deec9346-ab34-4b8a-9c5a-64e9e5f79559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>space_tokenized_text</th>\n",
       "      <th>rule_tokenized_text</th>\n",
       "      <th>wordpiece_tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>proces however afforded means ascertaining dim...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>['proces', 'however', 'afforded', 'means', 'as...</td>\n",
       "      <td>['proces', 'however', 'afforded', 'means', 'as...</td>\n",
       "      <td>['[CLS]', 'pro', '##ces', 'however', 'afforded...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>never occurred fumbling might mere mistake</td>\n",
       "      <td>HPL</td>\n",
       "      <td>['never', 'occurred', 'fumbling', 'might', 'me...</td>\n",
       "      <td>['never', 'occurred', 'fumbling', 'might', 'me...</td>\n",
       "      <td>['[CLS]', 'never', 'occurred', 'fu', '##mbling...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>left hand gold snuff box capered hil cutting m...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>['left', 'hand', 'gold', 'snuff', 'box', 'cape...</td>\n",
       "      <td>['left', 'hand', 'gold', 'snuff', 'box', 'cape...</td>\n",
       "      <td>['[CLS]', 'left', 'hand', 'gold', 's', '##nu',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>lovely spring looked windsor terrace sixteen f...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>['lovely', 'spring', 'looked', 'windsor', 'ter...</td>\n",
       "      <td>['lovely', 'spring', 'looked', 'windsor', 'ter...</td>\n",
       "      <td>['[CLS]', 'lovely', 'spring', 'looked', 'winds...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>finding nothing else even gold superintendent ...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>['finding', 'nothing', 'else', 'even', 'gold',...</td>\n",
       "      <td>['finding', 'nothing', 'else', 'even', 'gold',...</td>\n",
       "      <td>['[CLS]', 'finding', 'nothing', 'else', 'even'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  proces however afforded means ascertaining dim...    EAP   \n",
       "1  id17569         never occurred fumbling might mere mistake    HPL   \n",
       "2  id11008  left hand gold snuff box capered hil cutting m...    EAP   \n",
       "3  id27763  lovely spring looked windsor terrace sixteen f...    MWS   \n",
       "4  id12958  finding nothing else even gold superintendent ...    HPL   \n",
       "\n",
       "                                space_tokenized_text  \\\n",
       "0  ['proces', 'however', 'afforded', 'means', 'as...   \n",
       "1  ['never', 'occurred', 'fumbling', 'might', 'me...   \n",
       "2  ['left', 'hand', 'gold', 'snuff', 'box', 'cape...   \n",
       "3  ['lovely', 'spring', 'looked', 'windsor', 'ter...   \n",
       "4  ['finding', 'nothing', 'else', 'even', 'gold',...   \n",
       "\n",
       "                                 rule_tokenized_text  \\\n",
       "0  ['proces', 'however', 'afforded', 'means', 'as...   \n",
       "1  ['never', 'occurred', 'fumbling', 'might', 'me...   \n",
       "2  ['left', 'hand', 'gold', 'snuff', 'box', 'cape...   \n",
       "3  ['lovely', 'spring', 'looked', 'windsor', 'ter...   \n",
       "4  ['finding', 'nothing', 'else', 'even', 'gold',...   \n",
       "\n",
       "                            wordpiece_tokenized_text  \n",
       "0  ['[CLS]', 'pro', '##ces', 'however', 'afforded...  \n",
       "1  ['[CLS]', 'never', 'occurred', 'fu', '##mbling...  \n",
       "2  ['[CLS]', 'left', 'hand', 'gold', 's', '##nu',...  \n",
       "3  ['[CLS]', 'lovely', 'spring', 'looked', 'winds...  \n",
       "4  ['[CLS]', 'finding', 'nothing', 'else', 'even'...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"spooky_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99e364-3a76-43f6-a545-9204c989ae86",
   "metadata": {},
   "source": [
    "# B. Encodage de la variable à prédire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7952bc05-3904-4cad-8e79-4528de3cc2d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>space_tokenized_text</th>\n",
       "      <th>rule_tokenized_text</th>\n",
       "      <th>wordpiece_tokenized_text</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>proces however afforded means ascertaining dim...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>['proces', 'however', 'afforded', 'means', 'as...</td>\n",
       "      <td>['proces', 'however', 'afforded', 'means', 'as...</td>\n",
       "      <td>['[CLS]', 'pro', '##ces', 'however', 'afforded...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>never occurred fumbling might mere mistake</td>\n",
       "      <td>HPL</td>\n",
       "      <td>['never', 'occurred', 'fumbling', 'might', 'me...</td>\n",
       "      <td>['never', 'occurred', 'fumbling', 'might', 'me...</td>\n",
       "      <td>['[CLS]', 'never', 'occurred', 'fu', '##mbling...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>left hand gold snuff box capered hil cutting m...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>['left', 'hand', 'gold', 'snuff', 'box', 'cape...</td>\n",
       "      <td>['left', 'hand', 'gold', 'snuff', 'box', 'cape...</td>\n",
       "      <td>['[CLS]', 'left', 'hand', 'gold', 's', '##nu',...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>lovely spring looked windsor terrace sixteen f...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>['lovely', 'spring', 'looked', 'windsor', 'ter...</td>\n",
       "      <td>['lovely', 'spring', 'looked', 'windsor', 'ter...</td>\n",
       "      <td>['[CLS]', 'lovely', 'spring', 'looked', 'winds...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>finding nothing else even gold superintendent ...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>['finding', 'nothing', 'else', 'even', 'gold',...</td>\n",
       "      <td>['finding', 'nothing', 'else', 'even', 'gold',...</td>\n",
       "      <td>['[CLS]', 'finding', 'nothing', 'else', 'even'...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author  \\\n",
       "0  id26305  proces however afforded means ascertaining dim...    EAP   \n",
       "1  id17569         never occurred fumbling might mere mistake    HPL   \n",
       "2  id11008  left hand gold snuff box capered hil cutting m...    EAP   \n",
       "3  id27763  lovely spring looked windsor terrace sixteen f...    MWS   \n",
       "4  id12958  finding nothing else even gold superintendent ...    HPL   \n",
       "\n",
       "                                space_tokenized_text  \\\n",
       "0  ['proces', 'however', 'afforded', 'means', 'as...   \n",
       "1  ['never', 'occurred', 'fumbling', 'might', 'me...   \n",
       "2  ['left', 'hand', 'gold', 'snuff', 'box', 'cape...   \n",
       "3  ['lovely', 'spring', 'looked', 'windsor', 'ter...   \n",
       "4  ['finding', 'nothing', 'else', 'even', 'gold',...   \n",
       "\n",
       "                                 rule_tokenized_text  \\\n",
       "0  ['proces', 'however', 'afforded', 'means', 'as...   \n",
       "1  ['never', 'occurred', 'fumbling', 'might', 'me...   \n",
       "2  ['left', 'hand', 'gold', 'snuff', 'box', 'cape...   \n",
       "3  ['lovely', 'spring', 'looked', 'windsor', 'ter...   \n",
       "4  ['finding', 'nothing', 'else', 'even', 'gold',...   \n",
       "\n",
       "                            wordpiece_tokenized_text  EAP  HPL  MWS  \n",
       "0  ['[CLS]', 'pro', '##ces', 'however', 'afforded...  1.0  0.0  0.0  \n",
       "1  ['[CLS]', 'never', 'occurred', 'fu', '##mbling...  0.0  1.0  0.0  \n",
       "2  ['[CLS]', 'left', 'hand', 'gold', 's', '##nu',...  1.0  0.0  0.0  \n",
       "3  ['[CLS]', 'lovely', 'spring', 'looked', 'winds...  0.0  0.0  1.0  \n",
       "4  ['[CLS]', 'finding', 'nothing', 'else', 'even'...  0.0  1.0  0.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# OneHot Encoding for the 'author' column\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "author_encoded = encoder.fit_transform(df[['author']])  # Needs to be 2D\n",
    "\n",
    "# Convert the encoded result into a DataFrame\n",
    "encoded_df = pd.DataFrame(\n",
    "    author_encoded,\n",
    "    columns=encoder.categories_[0],  # Use the unique categories as column names\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Add the OneHotEncoded columns back to the original DataFrame\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b112503-e09e-4ec4-8f7a-4fd133197f75",
   "metadata": {},
   "source": [
    "# C. Construction des bases d’entraînement et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5433eafc-c859-4cd2-9686-9dc0d21238c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['text']\n",
    "y = df['author']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "#smote ,nn, smote nn, undersampling, oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8baff6d2-3b01-4f2b-96ba-39bd6eee2a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in the original dataset:\n",
      "author\n",
      "EAP    7899\n",
      "MWS    6044\n",
      "HPL    5634\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in the training dataset:\n",
      "author\n",
      "EAP    5479\n",
      "MWS    4297\n",
      "HPL    3927\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in the test dataset:\n",
      "author\n",
      "EAP    2420\n",
      "MWS    1747\n",
      "HPL    1707\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class distribution in the original dataset:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "print(\"\\nClass distribution in the training dataset:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"\\nClass distribution in the test dataset:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1597b7-835f-4fd9-817e-ff4d959b2c8b",
   "metadata": {},
   "source": [
    "# D. Méthodes de vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c42b042-a4d9-4577-bd01-94881b57c215",
   "metadata": {},
   "source": [
    "## 1. Frequence lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a47c3e3-2590-4ca9-a62f-18334dff208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train)\n",
    "X_test_cv = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37394858-9291-4157-974a-396560664773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vectorisation par fréquence lexicale (CountVectorizer) :\n",
      "Forme de X_train_counts : (13703, 22919)\n",
      "Forme de X_test_counts  : (5874, 22919)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVectorisation par fréquence lexicale (CountVectorizer) :\")\n",
    "print(\"Forme de X_train_counts :\", X_train_cv.shape)\n",
    "print(\"Forme de X_test_counts  :\", X_test_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb9f4a-bcbe-4126-b49c-777a935de6e8",
   "metadata": {},
   "source": [
    "X_train_cv contient des matrices de fréquence de mots, où chaque ligne représente un texte du dataset et chaque colonne correspond à un mot du vocabulaire extrait par CountVectorizer. La valeur dans la matrice indique combien de fois ce mot apparaît dans la ligne (document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c8833e3-5682-41da-a7e3-dee610b225c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abased</th>\n",
       "      <th>abasement</th>\n",
       "      <th>...</th>\n",
       "      <th>zobna</th>\n",
       "      <th>zobnarian</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zodiacal</th>\n",
       "      <th>zoilus</th>\n",
       "      <th>zokar</th>\n",
       "      <th>zone</th>\n",
       "      <th>zopyrus</th>\n",
       "      <th>zory</th>\n",
       "      <th>zubmizion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ab  aback  abaft  abandon  abandoned  abandoning  abandonment  abaout  \\\n",
       "0   0      0      0        0          0           0            0       0   \n",
       "1   0      0      0        0          0           0            0       0   \n",
       "2   0      0      0        0          0           0            0       0   \n",
       "3   0      0      0        0          0           0            0       0   \n",
       "4   0      0      0        0          0           0            0       0   \n",
       "\n",
       "   abased  abasement  ...  zobna  zobnarian  zodiac  zodiacal  zoilus  zokar  \\\n",
       "0       0          0  ...      0          0       0         0       0      0   \n",
       "1       0          0  ...      0          0       0         0       0      0   \n",
       "2       0          0  ...      0          0       0         0       0      0   \n",
       "3       0          0  ...      0          0       0         0       0      0   \n",
       "4       0          0  ...      0          0       0         0       0      0   \n",
       "\n",
       "   zone  zopyrus  zory  zubmizion  \n",
       "0     0        0     0          0  \n",
       "1     0        0     0          0  \n",
       "2     0        0     0          0  \n",
       "3     0        0     0          0  \n",
       "4     0        0     0          0  \n",
       "\n",
       "[5 rows x 22919 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cv = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names_out())\n",
    "df_cv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5197d666-7854-4846-a2e9-823a2379f11f",
   "metadata": {},
   "source": [
    "## 2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b51c439c-82a1-4f25-b6f8-2c63998df4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)  # On ajuste et transforme X_train une seule fois\n",
    "X_test_tfidf = tfidf.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "721f229a-11a8-49b3-bcd6-e136c8c0e1f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ab</th>\n",
       "      <th>aback</th>\n",
       "      <th>abaft</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abaout</th>\n",
       "      <th>abased</th>\n",
       "      <th>abasement</th>\n",
       "      <th>...</th>\n",
       "      <th>zobna</th>\n",
       "      <th>zobnarian</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zodiacal</th>\n",
       "      <th>zoilus</th>\n",
       "      <th>zokar</th>\n",
       "      <th>zone</th>\n",
       "      <th>zopyrus</th>\n",
       "      <th>zory</th>\n",
       "      <th>zubmizion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22919 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ab  aback  abaft  abandon  abandoned  abandoning  abandonment  abaout  \\\n",
       "0  0.0    0.0    0.0      0.0        0.0         0.0          0.0     0.0   \n",
       "1  0.0    0.0    0.0      0.0        0.0         0.0          0.0     0.0   \n",
       "2  0.0    0.0    0.0      0.0        0.0         0.0          0.0     0.0   \n",
       "3  0.0    0.0    0.0      0.0        0.0         0.0          0.0     0.0   \n",
       "4  0.0    0.0    0.0      0.0        0.0         0.0          0.0     0.0   \n",
       "\n",
       "   abased  abasement  ...  zobna  zobnarian  zodiac  zodiacal  zoilus  zokar  \\\n",
       "0     0.0        0.0  ...    0.0        0.0     0.0       0.0     0.0    0.0   \n",
       "1     0.0        0.0  ...    0.0        0.0     0.0       0.0     0.0    0.0   \n",
       "2     0.0        0.0  ...    0.0        0.0     0.0       0.0     0.0    0.0   \n",
       "3     0.0        0.0  ...    0.0        0.0     0.0       0.0     0.0    0.0   \n",
       "4     0.0        0.0  ...    0.0        0.0     0.0       0.0     0.0    0.0   \n",
       "\n",
       "   zone  zopyrus  zory  zubmizion  \n",
       "0   0.0      0.0   0.0        0.0  \n",
       "1   0.0      0.0   0.0        0.0  \n",
       "2   0.0      0.0   0.0        0.0  \n",
       "3   0.0      0.0   0.0        0.0  \n",
       "4   0.0      0.0   0.0        0.0  \n",
       "\n",
       "[5 rows x 22919 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf = pd.DataFrame(X_train_tfidf.toarray(), columns=tfidf.get_feature_names_out())\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc84f02-3e85-4292-8585-8d8002fe54a7",
   "metadata": {},
   "source": [
    "# Smote pour équilibré la dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d09e0be3-315a-4973-a419-f2082ab35c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_cv_balanced, y_train_cv_balanced = smote.fit_resample(X_train_cv, y_train)\n",
    "X_train_tfidf_balanced, y_train_tfidf_balanced = smote.fit_resample(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd6931-489b-495a-9c9a-fb45df62b29d",
   "metadata": {},
   "source": [
    "# E. Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9434a5-8ae4-44dd-baba-01cb7a9990a6",
   "metadata": {},
   "source": [
    "### 1. Créer trois modèles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d8ecc0c-a7ac-4cf9-b690-9bca44a2b00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp1_cv = MLPClassifier(hidden_layer_sizes=(100,), max_iter=5, activation='relu', solver='adam', random_state=0)\n",
    "mlp2_cv = MLPClassifier(hidden_layer_sizes=(100,), max_iter=5, activation='logistic', solver='sgd', random_state=0)\n",
    "mlp3_cv = MLPClassifier(hidden_layer_sizes=(100,), max_iter=5, activation='tanh', solver='adam', random_state=0)\n",
    "\n",
    "mlp1_tfidf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=5, activation='relu', solver='adam', random_state=0)\n",
    "mlp2_tfidf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=5, activation='logistic', solver='sgd', random_state=0)\n",
    "mlp3_tfidf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=5, activation='tanh', solver='adam', random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b8a38c-eadc-4f4a-831d-6c6ad175c434",
   "metadata": {},
   "source": [
    "### 2.Entraînement sur les représentations vectorielles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e75172-9188-47b7-bc5a-edbdbf72e6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, max_iter=5, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, max_iter=5, random_state=0)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', max_iter=5, random_state=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp1_cv.fit(X_train_cv_balanced, y_train_cv_balanced)\n",
    "mlp2_cv.fit(X_train_cv_balanced, y_train_cv_balanced)\n",
    "mlp3_cv.fit(X_train_cv_balanced, y_train_cv_balanced)\n",
    "\n",
    "mlp1_tfidf.fit(X_train_tfidf_balanced, y_train_tfidf_balanced)\n",
    "mlp2_tfidf.fit(X_train_tfidf_balanced, y_train_tfidf_balanced)\n",
    "mlp3_tfidf.fit(X_train_tfidf_balanced, y_train_tfidf_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbba2ce0-a49d-4e48-9250-1ec2ec7bc0ee",
   "metadata": {},
   "source": [
    "### 3. Prédiction et Évaluation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2a3f58b-ca73-4d00-8faf-8b3843273869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP1 + CountVectorizer:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.99      0.97      0.98      5479\n",
      "         HPL       0.98      0.99      0.98      3927\n",
      "         MWS       0.98      0.99      0.99      4297\n",
      "\n",
      "    accuracy                           0.99     13703\n",
      "   macro avg       0.98      0.99      0.99     13703\n",
      "weighted avg       0.99      0.99      0.99     13703\n",
      "\n",
      "MLP2 + CountVectorizer:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.40      1.00      0.57      5479\n",
      "         HPL       0.00      0.00      0.00      3927\n",
      "         MWS       0.55      0.01      0.02      4297\n",
      "\n",
      "    accuracy                           0.40     13703\n",
      "   macro avg       0.32      0.34      0.20     13703\n",
      "weighted avg       0.33      0.40      0.24     13703\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP3 + CountVectorizer:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.99      0.97      0.98      5479\n",
      "         HPL       0.98      0.99      0.99      3927\n",
      "         MWS       0.98      0.99      0.99      4297\n",
      "\n",
      "    accuracy                           0.99     13703\n",
      "   macro avg       0.98      0.99      0.99     13703\n",
      "weighted avg       0.99      0.99      0.99     13703\n",
      "\n",
      "MLP1 + TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.99      0.99      0.99      5479\n",
      "         HPL       0.99      0.99      0.99      3927\n",
      "         MWS       0.99      0.99      0.99      4297\n",
      "\n",
      "    accuracy                           0.99     13703\n",
      "   macro avg       0.99      0.99      0.99     13703\n",
      "weighted avg       0.99      0.99      0.99     13703\n",
      "\n",
      "MLP2 + TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.40      1.00      0.57      5479\n",
      "         HPL       0.00      0.00      0.00      3927\n",
      "         MWS       0.00      0.00      0.00      4297\n",
      "\n",
      "    accuracy                           0.40     13703\n",
      "   macro avg       0.13      0.33      0.19     13703\n",
      "weighted avg       0.16      0.40      0.23     13703\n",
      "\n",
      "MLP3 + TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.99      0.99      0.99      5479\n",
      "         HPL       0.99      0.99      0.99      3927\n",
      "         MWS       0.99      0.99      0.99      4297\n",
      "\n",
      "    accuracy                           0.99     13703\n",
      "   macro avg       0.99      0.99      0.99     13703\n",
      "weighted avg       0.99      0.99      0.99     13703\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred1 = mlp1_cv.predict(X_train_cv)\n",
    "y_pred2 = mlp2_cv.predict(X_train_cv)\n",
    "y_pred3 = mlp3_cv.predict(X_train_cv)\n",
    "print(\"MLP1 + CountVectorizer:\\n\", classification_report(y_train, y_pred1))\n",
    "print(\"MLP2 + CountVectorizer:\\n\", classification_report(y_train, y_pred2))\n",
    "print(\"MLP3 + CountVectorizer:\\n\", classification_report(y_train, y_pred3))\n",
    "\n",
    "y2_pred1 = mlp1_tfidf.predict(X_train_tfidf)\n",
    "y2_pred2 = mlp2_tfidf.predict(X_train_tfidf)\n",
    "y2_pred3 = mlp3_tfidf.predict(X_train_tfidf)\n",
    "print(\"MLP1 + TF-IDF:\\n\", classification_report(y_train, y2_pred1))\n",
    "print(\"MLP2 + TF-IDF:\\n\", classification_report(y_train, y2_pred2))\n",
    "print(\"MLP3 + TF-IDF:\\n\", classification_report(y_train, y2_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad896174-9a7f-4d12-b24e-0512ce763765",
   "metadata": {},
   "source": [
    "# F. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8762f14-5246-4ae0-ac4d-a17e491c896c",
   "metadata": {},
   "source": [
    "### 1. Predict on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dbeadf0-6a01-4d64-9c74-426920ca3e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP1 + CountVectorizer:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.81      0.76      0.79      2420\n",
      "         HPL       0.76      0.79      0.78      1707\n",
      "         MWS       0.76      0.80      0.78      1747\n",
      "\n",
      "    accuracy                           0.78      5874\n",
      "   macro avg       0.78      0.78      0.78      5874\n",
      "weighted avg       0.78      0.78      0.78      5874\n",
      "\n",
      "MLP2 + CountVectorizer:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.41      1.00      0.58      2420\n",
      "         HPL       1.00      0.00      0.00      1707\n",
      "         MWS       0.47      0.01      0.02      1747\n",
      "\n",
      "    accuracy                           0.41      5874\n",
      "   macro avg       0.63      0.34      0.20      5874\n",
      "weighted avg       0.60      0.41      0.25      5874\n",
      "\n",
      "MLP3 + CountVectorizer:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.81      0.76      0.78      2420\n",
      "         HPL       0.76      0.79      0.77      1707\n",
      "         MWS       0.76      0.80      0.78      1747\n",
      "\n",
      "    accuracy                           0.78      5874\n",
      "   macro avg       0.78      0.78      0.78      5874\n",
      "weighted avg       0.78      0.78      0.78      5874\n",
      "\n",
      "MLP1 + TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.83      0.84      0.84      2420\n",
      "         HPL       0.83      0.82      0.82      1707\n",
      "         MWS       0.82      0.82      0.82      1747\n",
      "\n",
      "    accuracy                           0.83      5874\n",
      "   macro avg       0.83      0.83      0.83      5874\n",
      "weighted avg       0.83      0.83      0.83      5874\n",
      "\n",
      "MLP2 + TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.41      1.00      0.58      2420\n",
      "         HPL       0.00      0.00      0.00      1707\n",
      "         MWS       0.00      0.00      0.00      1747\n",
      "\n",
      "    accuracy                           0.41      5874\n",
      "   macro avg       0.14      0.33      0.19      5874\n",
      "weighted avg       0.17      0.41      0.24      5874\n",
      "\n",
      "MLP3 + TF-IDF:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         EAP       0.83      0.84      0.84      2420\n",
      "         HPL       0.83      0.82      0.82      1707\n",
      "         MWS       0.82      0.82      0.82      1747\n",
      "\n",
      "    accuracy                           0.83      5874\n",
      "   macro avg       0.83      0.83      0.83      5874\n",
      "weighted avg       0.83      0.83      0.83      5874\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred1 = mlp1_cv.predict(X_test_cv)\n",
    "y_pred2 = mlp2_cv.predict(X_test_cv)\n",
    "y_pred3 = mlp3_cv.predict(X_test_cv)\n",
    "print(\"MLP1 + CountVectorizer:\\n\", classification_report(y_test, y_pred1))\n",
    "print(\"MLP2 + CountVectorizer:\\n\", classification_report(y_test, y_pred2))\n",
    "print(\"MLP3 + CountVectorizer:\\n\", classification_report(y_test, y_pred3))\n",
    "\n",
    "y2_pred1 = mlp1_tfidf.predict(X_test_tfidf)\n",
    "y2_pred2 = mlp2_tfidf.predict(X_test_tfidf)\n",
    "y2_pred3 = mlp3_tfidf.predict(X_test_tfidf)\n",
    "print(\"MLP1 + TF-IDF:\\n\", classification_report(y_test, y2_pred1))\n",
    "print(\"MLP2 + TF-IDF:\\n\", classification_report(y_test, y2_pred2))\n",
    "print(\"MLP3 + TF-IDF:\\n\", classification_report(y_test, y2_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed7b93-8064-4dcb-8e89-929ed564238a",
   "metadata": {},
   "source": [
    "### 2. Compute Prediction Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b8ed7ca-1913-4446-8ffa-5bf90b4d067e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Time (MLP1 + CountVectorizer): 0.0358 seconds\n",
      "Prediction Time (MLP2 + CountVectorizer): 0.0595 seconds\n",
      "Prediction Time (MLP3 + CountVectorizer): 0.0249 seconds\n",
      "Prediction Time (MLP1 + TF-IDF:): 0.0119 seconds\n",
      "Prediction Time (MLP2 + TF-IDF): 0.0412 seconds\n",
      "Prediction Time (MLP3 + TF-IDF): 0.0250 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def time_prediction(model, X):\n",
    "    start = time.time()\n",
    "    model.predict(X)\n",
    "    end = time.time()\n",
    "    return end - start\n",
    "\n",
    "time_mlp1_cv = time_prediction(mlp1_cv, X_test_cv)\n",
    "time_mlp2_cv = time_prediction(mlp2_cv, X_test_cv)\n",
    "time_mlp3_cv = time_prediction(mlp3_cv, X_test_cv)\n",
    "\n",
    "print(f\"Prediction Time (MLP1 + CountVectorizer): {time_mlp1_cv:.4f} seconds\")\n",
    "print(f\"Prediction Time (MLP2 + CountVectorizer): {time_mlp2_cv:.4f} seconds\")\n",
    "print(f\"Prediction Time (MLP3 + CountVectorizer): {time_mlp3_cv:.4f} seconds\")\n",
    "\n",
    "\n",
    "time_mlp1_tfidf = time_prediction(mlp1_tfidf, X_test_tfidf)\n",
    "time_mlp2_tfidf = time_prediction(mlp2_tfidf, X_test_tfidf)\n",
    "time_mlp3_tfidf = time_prediction(mlp3_tfidf, X_test_tfidf)\n",
    "\n",
    "print(f\"Prediction Time (MLP1 + TF-IDF:): {time_mlp1_tfidf:.4f} seconds\")\n",
    "print(f\"Prediction Time (MLP2 + TF-IDF): {time_mlp2_tfidf:.4f} seconds\")\n",
    "print(f\"Prediction Time (MLP3 + TF-IDF): {time_mlp3_tfidf:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e595adda-1bed-4dd9-be5a-8d04497fe420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "models = [\n",
    "    (mlp2_cv, \"logistic\", \"CountVectorizer\"),\n",
    "    (mlp3_cv, \"tanh\", \"CountVectorizer\"),\n",
    "    (mlp2_tfidf, \"logistic\", \"TF-IDF\"),\n",
    "    (mlp3_tfidf, \"tanh\", \"TF-IDF\")\n",
    "]\n",
    "\n",
    "test_sets = {\n",
    "    \"CountVectorizer\": X_test_cv,\n",
    "    \"TF-IDF\": X_test_tfidf\n",
    "}\n",
    "\n",
    "for model, activation, vectorizer in models:\n",
    "    y_pred = model.predict(test_sets[vectorizer])\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    evaluation_results.append({\n",
    "        'Model': activation,\n",
    "        'Vectorizer': vectorizer,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1\n",
    "    })\n",
    "\n",
    "evaluation = pd.DataFrame(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "365997bb-b763-49aa-8ce8-f2efa9b7e503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.413687</td>\n",
       "      <td>0.602092</td>\n",
       "      <td>0.413687</td>\n",
       "      <td>0.247276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tanh</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.779707</td>\n",
       "      <td>0.780978</td>\n",
       "      <td>0.779707</td>\n",
       "      <td>0.779796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.411985</td>\n",
       "      <td>0.169732</td>\n",
       "      <td>0.411985</td>\n",
       "      <td>0.240416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.828056</td>\n",
       "      <td>0.828061</td>\n",
       "      <td>0.828056</td>\n",
       "      <td>0.828023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model       Vectorizer  Accuracy  Precision    Recall  F1-score\n",
       "0  logistic  CountVectorizer  0.413687   0.602092  0.413687  0.247276\n",
       "1      tanh  CountVectorizer  0.779707   0.780978  0.779707  0.779796\n",
       "2  logistic           TF-IDF  0.411985   0.169732  0.411985  0.240416\n",
       "3      tanh           TF-IDF  0.828056   0.828061  0.828056  0.828023"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a3dc3-07b3-4ff9-bf98-93de42de732d",
   "metadata": {},
   "source": [
    "# G. Vectorisations basées sur les embeddings de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8dde8e0-4e4f-4085-9254-23e16e865716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hafsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7de7ccd0-c930-4f0b-9545-398f7b72213e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 9.2103\n",
      "Epoch 2/3, Loss: 9.2103\n",
      "Epoch 3/3, Loss: 9.2100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "\n",
    "texts = df['text'].tolist()\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "tokenized_texts = [preprocess(text) for text in texts]\n",
    "word_counts = Counter(word for text in tokenized_texts for word in text)\n",
    "vocab = [word for word, count in word_counts.most_common(10000)]\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# create training data where each sample represent a target and its context\n",
    "window_size = 3\n",
    "training_data = []\n",
    "for text in tokenized_texts:\n",
    "    indices = [word_to_idx[word] for word in text if word in word_to_idx]\n",
    "    for i, target in enumerate(indices):\n",
    "        context = []\n",
    "        for j in range(i-window_size, i+window_size+1):\n",
    "            if j != i and 0 <= j < len(indices):\n",
    "                context.append(indices[j])\n",
    "        if len(context) > 0:\n",
    "            training_data.append((context, target))\n",
    "\n",
    "\n",
    "\n",
    "embedding_dim = 10\n",
    "learning_rate = 0.01\n",
    "epochs = 3\n",
    "\n",
    "\n",
    "\n",
    "W1 = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "W2 = np.random.randn(embedding_dim, vocab_size) * 0.01\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "#input is the context and the output is the target , this is CBOW\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    # Forward Pass\n",
    "    for context, target in training_data:\n",
    "        context_vectors = W1[context, :] # we have W1 contain for each word its embedding\n",
    "        hidden = np.mean(context_vectors, axis=0)\n",
    "        output = np.dot(hidden, W2)  \n",
    "        probs = softmax(output)\n",
    "\n",
    "\n",
    "        target_onehot = np.zeros(vocab_size)\n",
    "        target_onehot[target] = 1\n",
    "        loss = -np.log(probs[target])  \n",
    "        total_loss += loss\n",
    "        # Backward Pass\n",
    "        grad_output = probs - target_onehot\n",
    "        grad_hidden = np.dot(W2, grad_output)\n",
    "        grad_W2 = np.outer(hidden, grad_output)\n",
    "        grad_context = grad_hidden / len(context)\n",
    "\n",
    "\n",
    "        for word_idx in context:\n",
    "            W1[word_idx] -= learning_rate * grad_context  \n",
    "        W2 -= learning_rate * grad_W2\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(training_data):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28d33c26-0c65-465b-ac17-1229e363048b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots similaires à 'spring':\n",
      "spring (similarité: 1.0000)\n",
      "dormant (similarité: 0.8800)\n",
      "welfare (similarité: 0.8550)\n",
      "worship (similarité: 0.8543)\n",
      "gibbous (similarité: 0.8487)\n",
      "emitting (similarité: 0.8448)\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(word):\n",
    "    return W1[word_to_idx[word]] if word in word_to_idx else None\n",
    "\n",
    "\n",
    "def find_similar_words(query_word, top_n=5):\n",
    "    if query_word not in word_to_idx:\n",
    "        return []\n",
    "\n",
    "    query_vec = W1[word_to_idx[query_word]]\n",
    "    similarities = []\n",
    "\n",
    "    for word, idx in word_to_idx.items():\n",
    "        vec = W1[idx]\n",
    "        sim = np.dot(query_vec, vec)/(np.linalg.norm(query_vec)*np.linalg.norm(vec))\n",
    "        similarities.append((word, sim))\n",
    "\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "\n",
    "\n",
    "word = \"spring\" \n",
    "similar_words = find_similar_words(word, top_n=6)\n",
    "\n",
    "print(f\"Mots similaires à '{word}':\")\n",
    "for w, score in similar_words:\n",
    "    print(f\"{w} (similarité: {score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a74df3-af32-4a5a-9a62-23e702b90f8a",
   "metadata": {},
   "source": [
    "### Glove and FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4f619265-a418-4253-b7b7-4243c7b8e0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hafsa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import FastText, KeyedVectors\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from joblib import dump\n",
    "nltk.download('punkt')\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e185f36-2e56-43c9-bff1-371c05a344d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(vectors, words):\n",
    "    words = [word for word in words if word in vectors]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(50)\n",
    "    return np.mean(vectors[words], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "317c70fe-8b7a-439a-bf93-0ec30a1c137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use FastText\n",
    "X_train_ft = [word_tokenize(text) for text in X_train]\n",
    "X_test_ft = [word_tokenize(text) for text in X_test]\n",
    "\n",
    "model = FastText(X_train_ft, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "X_train_ft = [vectorize(model.wv, sentence) for sentence in X_train_ft]\n",
    "X_test_ft = [vectorize(model.wv, sentence) for sentence in X_test_ft]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e3aa555-9a42-4e30-bec5-25076011cb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use glove\n",
    "X_train_gl = [word_tokenize(text) for text in X_train]\n",
    "X_test_gl = [word_tokenize(text) for text in X_test]\n",
    "\n",
    "glove = KeyedVectors.load_word2vec_format('glove.6B.50d.txt', binary=False, no_header=True)\n",
    "\n",
    "X_train_glove = [vectorize(glove, sentence) for sentence in X_train_gl]\n",
    "X_test_glove = [vectorize(glove, sentence) for sentence in X_test_gl]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b53ddad-d8ea-4900-9c12-c49b4a11b288",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ee0974d-1e49-4438-b763-64d267445564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "X_train_w2v = [word_tokenize(text) for text in X_train]\n",
    "X_test_w2v = [word_tokenize(text) for text in X_test]\n",
    "\n",
    "w2v_model = Word2Vec(sentences=X_train_w2v, vector_size=50, window=5, min_count=1, workers=4)\n",
    "\n",
    "def vectorize_w2v(model, words):\n",
    "    words = [word for word in words if word in model.wv]\n",
    "    if len(words) == 0:\n",
    "        return np.zeros(50)\n",
    "    return np.mean(model.wv[words], axis=0)\n",
    "\n",
    "X_train_w2v = [vectorize_w2v(w2v_model, sentence) for sentence in X_train_w2v]\n",
    "X_test_w2v = [vectorize_w2v(w2v_model, sentence) for sentence in X_test_w2v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba9b8dac-505e-43fa-a74b-264233183c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vectorizer, model, epochs=10):\n",
    "    X = None\n",
    "    y = y_train\n",
    "\n",
    "    if vectorizer == 'ft':\n",
    "        X = X_train_ft\n",
    "    elif vectorizer == 'gl':\n",
    "        X = X_train_glove\n",
    "    elif vectorizer == 'w2v':\n",
    "        X = X_train_w2v\n",
    "    else:\n",
    "        raise ValueError(\"Invalid vectorizer\")\n",
    "\n",
    "    losses = []\n",
    "    prev_accuracy = None \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.partial_fit(X, y, classes=np.unique(y_train))\n",
    "\n",
    "        y_train_pred = model.predict(X)\n",
    "\n",
    "        accuracy = accuracy_score(y, y_train_pred)\n",
    "        precision = precision_score(y, y_train_pred, average='weighted', zero_division=0)\n",
    "        recall = recall_score(y, y_train_pred, average='weighted')\n",
    "        f1 = f1_score(y, y_train_pred, average='weighted')\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "        \n",
    "        if prev_accuracy is not None:\n",
    "            loss = accuracy - prev_accuracy\n",
    "            losses.append(loss)\n",
    "        prev_accuracy = accuracy\n",
    "\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "    dump(model, f'models/{model.activation}_{vectorizer}.joblib')\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b770ea9-3b40-4573-b666-ec11a945f52c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with logistic activation and ft vectorizer\n",
      "Epoch 1/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 2/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 3/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 4/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 5/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 6/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 7/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 8/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 9/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 10/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 11/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 12/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 13/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 14/100 - Accuracy: 0.3989, Precision: 0.1771, Recall: 0.3989, F1-score: 0.2282\n",
      "Epoch 15/100 - Accuracy: 0.3986, Precision: 0.2001, Recall: 0.3986, F1-score: 0.2285\n",
      "Epoch 16/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 17/100 - Accuracy: 0.3993, Precision: 0.1859, Recall: 0.3993, F1-score: 0.2283\n",
      "Epoch 18/100 - Accuracy: 0.3993, Precision: 0.1598, Recall: 0.3993, F1-score: 0.2282\n",
      "Epoch 19/100 - Accuracy: 0.3945, Precision: 0.2456, Recall: 0.3945, F1-score: 0.2370\n",
      "Epoch 20/100 - Accuracy: 0.3984, Precision: 0.1998, Recall: 0.3984, F1-score: 0.2286\n",
      "Epoch 21/100 - Accuracy: 0.3955, Precision: 0.2418, Recall: 0.3955, F1-score: 0.2337\n",
      "Epoch 22/100 - Accuracy: 0.3955, Precision: 0.2378, Recall: 0.3955, F1-score: 0.2330\n",
      "Epoch 23/100 - Accuracy: 0.3973, Precision: 0.2297, Recall: 0.3973, F1-score: 0.2303\n",
      "Epoch 24/100 - Accuracy: 0.3964, Precision: 0.2396, Recall: 0.3964, F1-score: 0.2321\n",
      "Epoch 25/100 - Accuracy: 0.3974, Precision: 0.2131, Recall: 0.3974, F1-score: 0.2293\n",
      "Epoch 26/100 - Accuracy: 0.3933, Precision: 0.2501, Recall: 0.3933, F1-score: 0.2407\n",
      "Epoch 27/100 - Accuracy: 0.3936, Precision: 0.2514, Recall: 0.3936, F1-score: 0.2413\n",
      "Epoch 28/100 - Accuracy: 0.3914, Precision: 0.2596, Recall: 0.3914, F1-score: 0.2552\n",
      "Epoch 29/100 - Accuracy: 0.3953, Precision: 0.2396, Recall: 0.3953, F1-score: 0.2334\n",
      "Epoch 30/100 - Accuracy: 0.3958, Precision: 0.2374, Recall: 0.3958, F1-score: 0.2324\n",
      "Epoch 31/100 - Accuracy: 0.3938, Precision: 0.2464, Recall: 0.3938, F1-score: 0.2383\n",
      "Epoch 32/100 - Accuracy: 0.3974, Precision: 0.2022, Recall: 0.3974, F1-score: 0.2287\n",
      "Epoch 33/100 - Accuracy: 0.3947, Precision: 0.2441, Recall: 0.3947, F1-score: 0.2362\n",
      "Epoch 34/100 - Accuracy: 0.3962, Precision: 0.2389, Recall: 0.3962, F1-score: 0.2323\n",
      "Epoch 35/100 - Accuracy: 0.3907, Precision: 0.2621, Recall: 0.3907, F1-score: 0.2620\n",
      "Epoch 36/100 - Accuracy: 0.3902, Precision: 0.2669, Recall: 0.3902, F1-score: 0.2774\n",
      "Epoch 37/100 - Accuracy: 0.3934, Precision: 0.2592, Recall: 0.3934, F1-score: 0.2486\n",
      "Epoch 38/100 - Accuracy: 0.3966, Precision: 0.2376, Recall: 0.3966, F1-score: 0.2317\n",
      "Epoch 39/100 - Accuracy: 0.3889, Precision: 0.2639, Recall: 0.3889, F1-score: 0.2734\n",
      "Epoch 40/100 - Accuracy: 0.3952, Precision: 0.2409, Recall: 0.3952, F1-score: 0.2339\n",
      "Epoch 41/100 - Accuracy: 0.3936, Precision: 0.2583, Recall: 0.3936, F1-score: 0.2462\n",
      "Epoch 42/100 - Accuracy: 0.3927, Precision: 0.2590, Recall: 0.3927, F1-score: 0.2529\n",
      "Epoch 43/100 - Accuracy: 0.3967, Precision: 0.2382, Recall: 0.3967, F1-score: 0.2318\n",
      "Epoch 44/100 - Accuracy: 0.3861, Precision: 0.2699, Recall: 0.3861, F1-score: 0.3083\n",
      "Epoch 45/100 - Accuracy: 0.3943, Precision: 0.2478, Recall: 0.3943, F1-score: 0.2380\n",
      "Epoch 46/100 - Accuracy: 0.3903, Precision: 0.2668, Recall: 0.3903, F1-score: 0.2770\n",
      "Epoch 47/100 - Accuracy: 0.3936, Precision: 0.2598, Recall: 0.3936, F1-score: 0.2490\n",
      "Epoch 48/100 - Accuracy: 0.3939, Precision: 0.2568, Recall: 0.3939, F1-score: 0.2440\n",
      "Epoch 49/100 - Accuracy: 0.3909, Precision: 0.2571, Recall: 0.3909, F1-score: 0.2535\n",
      "Epoch 50/100 - Accuracy: 0.3878, Precision: 0.2679, Recall: 0.3878, F1-score: 0.2921\n",
      "Epoch 51/100 - Accuracy: 0.3881, Precision: 0.2684, Recall: 0.3881, F1-score: 0.2939\n",
      "Epoch 52/100 - Accuracy: 0.3952, Precision: 0.2398, Recall: 0.3952, F1-score: 0.2336\n",
      "Epoch 53/100 - Accuracy: 0.3926, Precision: 0.2580, Recall: 0.3926, F1-score: 0.2502\n",
      "Epoch 54/100 - Accuracy: 0.3925, Precision: 0.2584, Recall: 0.3925, F1-score: 0.2524\n",
      "Epoch 55/100 - Accuracy: 0.3894, Precision: 0.2621, Recall: 0.3894, F1-score: 0.2689\n",
      "Epoch 56/100 - Accuracy: 0.3968, Precision: 0.2394, Recall: 0.3968, F1-score: 0.2318\n",
      "Epoch 57/100 - Accuracy: 0.3975, Precision: 0.2164, Recall: 0.3975, F1-score: 0.2294\n",
      "Epoch 58/100 - Accuracy: 0.3909, Precision: 0.2594, Recall: 0.3909, F1-score: 0.2556\n",
      "Epoch 59/100 - Accuracy: 0.3939, Precision: 0.2568, Recall: 0.3939, F1-score: 0.2440\n",
      "Epoch 60/100 - Accuracy: 0.3933, Precision: 0.2560, Recall: 0.3933, F1-score: 0.2452\n",
      "Epoch 61/100 - Accuracy: 0.3881, Precision: 0.2684, Recall: 0.3881, F1-score: 0.2932\n",
      "Epoch 62/100 - Accuracy: 0.3901, Precision: 0.2613, Recall: 0.3901, F1-score: 0.2625\n",
      "Epoch 63/100 - Accuracy: 0.3893, Precision: 0.2604, Recall: 0.3893, F1-score: 0.2651\n",
      "Epoch 64/100 - Accuracy: 0.3914, Precision: 0.2615, Recall: 0.3914, F1-score: 0.2575\n",
      "Epoch 65/100 - Accuracy: 0.3933, Precision: 0.2503, Recall: 0.3933, F1-score: 0.2413\n",
      "Epoch 66/100 - Accuracy: 0.3902, Precision: 0.2670, Recall: 0.3902, F1-score: 0.2779\n",
      "Epoch 67/100 - Accuracy: 0.3896, Precision: 0.2614, Recall: 0.3896, F1-score: 0.2668\n",
      "Epoch 68/100 - Accuracy: 0.3911, Precision: 0.2599, Recall: 0.3911, F1-score: 0.2560\n",
      "Epoch 69/100 - Accuracy: 0.3890, Precision: 0.2626, Recall: 0.3890, F1-score: 0.2712\n",
      "Epoch 70/100 - Accuracy: 0.3936, Precision: 0.2598, Recall: 0.3936, F1-score: 0.2490\n",
      "Epoch 71/100 - Accuracy: 0.3896, Precision: 0.2666, Recall: 0.3896, F1-score: 0.2794\n",
      "Epoch 72/100 - Accuracy: 0.3914, Precision: 0.2619, Recall: 0.3914, F1-score: 0.2579\n",
      "Epoch 73/100 - Accuracy: 0.3918, Precision: 0.2632, Recall: 0.3918, F1-score: 0.2609\n",
      "Epoch 74/100 - Accuracy: 0.3882, Precision: 0.2675, Recall: 0.3882, F1-score: 0.2884\n",
      "Epoch 75/100 - Accuracy: 0.3955, Precision: 0.2348, Recall: 0.3955, F1-score: 0.2324\n",
      "Epoch 76/100 - Accuracy: 0.3887, Precision: 0.2694, Recall: 0.3887, F1-score: 0.2975\n",
      "Epoch 77/100 - Accuracy: 0.3946, Precision: 0.2421, Recall: 0.3946, F1-score: 0.2355\n",
      "Epoch 78/100 - Accuracy: 0.3896, Precision: 0.2709, Recall: 0.3896, F1-score: 0.3014\n",
      "Epoch 79/100 - Accuracy: 0.3898, Precision: 0.2615, Recall: 0.3898, F1-score: 0.2648\n",
      "Epoch 80/100 - Accuracy: 0.3936, Precision: 0.2591, Recall: 0.3936, F1-score: 0.2477\n",
      "Epoch 81/100 - Accuracy: 0.3909, Precision: 0.2572, Recall: 0.3909, F1-score: 0.2536\n",
      "Epoch 82/100 - Accuracy: 0.3913, Precision: 0.2608, Recall: 0.3913, F1-score: 0.2571\n",
      "Epoch 83/100 - Accuracy: 0.3896, Precision: 0.2710, Recall: 0.3896, F1-score: 0.3019\n",
      "Epoch 84/100 - Accuracy: 0.3896, Precision: 0.2619, Recall: 0.3896, F1-score: 0.2681\n",
      "Epoch 85/100 - Accuracy: 0.3895, Precision: 0.2624, Recall: 0.3895, F1-score: 0.2693\n",
      "Epoch 86/100 - Accuracy: 0.3916, Precision: 0.2624, Recall: 0.3916, F1-score: 0.2601\n",
      "Epoch 87/100 - Accuracy: 0.3915, Precision: 0.2576, Recall: 0.3915, F1-score: 0.2530\n",
      "Epoch 88/100 - Accuracy: 0.3925, Precision: 0.2581, Recall: 0.3925, F1-score: 0.2519\n",
      "Epoch 89/100 - Accuracy: 0.3877, Precision: 0.2734, Recall: 0.3877, F1-score: 0.3176\n",
      "Epoch 90/100 - Accuracy: 0.3917, Precision: 0.2638, Recall: 0.3917, F1-score: 0.2615\n",
      "Epoch 91/100 - Accuracy: 0.3887, Precision: 0.2680, Recall: 0.3887, F1-score: 0.2893\n",
      "Epoch 92/100 - Accuracy: 0.3927, Precision: 0.2587, Recall: 0.3927, F1-score: 0.2507\n",
      "Epoch 93/100 - Accuracy: 0.3926, Precision: 0.2583, Recall: 0.3926, F1-score: 0.2508\n",
      "Epoch 94/100 - Accuracy: 0.3936, Precision: 0.2592, Recall: 0.3936, F1-score: 0.2477\n",
      "Epoch 95/100 - Accuracy: 0.3884, Precision: 0.2666, Recall: 0.3884, F1-score: 0.2843\n",
      "Epoch 96/100 - Accuracy: 0.3896, Precision: 0.2615, Recall: 0.3896, F1-score: 0.2667\n",
      "Epoch 97/100 - Accuracy: 0.3938, Precision: 0.2458, Recall: 0.3938, F1-score: 0.2381\n",
      "Epoch 98/100 - Accuracy: 0.3894, Precision: 0.2612, Recall: 0.3894, F1-score: 0.2669\n",
      "Epoch 99/100 - Accuracy: 0.3901, Precision: 0.2669, Recall: 0.3901, F1-score: 0.2775\n",
      "Epoch 100/100 - Accuracy: 0.3960, Precision: 0.2388, Recall: 0.3960, F1-score: 0.2324\n",
      "Training model with logistic activation and gl vectorizer\n",
      "Epoch 1/100 - Accuracy: 0.4049, Precision: 0.2874, Recall: 0.4049, F1-score: 0.3271\n",
      "Epoch 2/100 - Accuracy: 0.4373, Precision: 0.4634, Recall: 0.4373, F1-score: 0.3538\n",
      "Epoch 3/100 - Accuracy: 0.4949, Precision: 0.5044, Recall: 0.4949, F1-score: 0.4655\n",
      "Epoch 4/100 - Accuracy: 0.5357, Precision: 0.5326, Recall: 0.5357, F1-score: 0.5262\n",
      "Epoch 5/100 - Accuracy: 0.5497, Precision: 0.5470, Recall: 0.5497, F1-score: 0.5426\n",
      "Epoch 6/100 - Accuracy: 0.5559, Precision: 0.5550, Recall: 0.5559, F1-score: 0.5469\n",
      "Epoch 7/100 - Accuracy: 0.5667, Precision: 0.5644, Recall: 0.5667, F1-score: 0.5622\n",
      "Epoch 8/100 - Accuracy: 0.5775, Precision: 0.5770, Recall: 0.5775, F1-score: 0.5771\n",
      "Epoch 9/100 - Accuracy: 0.5762, Precision: 0.5754, Recall: 0.5762, F1-score: 0.5723\n",
      "Epoch 10/100 - Accuracy: 0.5829, Precision: 0.5815, Recall: 0.5829, F1-score: 0.5812\n",
      "Epoch 11/100 - Accuracy: 0.5810, Precision: 0.5797, Recall: 0.5810, F1-score: 0.5783\n",
      "Epoch 12/100 - Accuracy: 0.5816, Precision: 0.5812, Recall: 0.5816, F1-score: 0.5785\n",
      "Epoch 13/100 - Accuracy: 0.5867, Precision: 0.5857, Recall: 0.5867, F1-score: 0.5857\n",
      "Epoch 14/100 - Accuracy: 0.5864, Precision: 0.5865, Recall: 0.5864, F1-score: 0.5857\n",
      "Epoch 15/100 - Accuracy: 0.5872, Precision: 0.5861, Recall: 0.5872, F1-score: 0.5859\n",
      "Epoch 16/100 - Accuracy: 0.5905, Precision: 0.5902, Recall: 0.5905, F1-score: 0.5903\n",
      "Epoch 17/100 - Accuracy: 0.5897, Precision: 0.5892, Recall: 0.5897, F1-score: 0.5889\n",
      "Epoch 18/100 - Accuracy: 0.5857, Precision: 0.5892, Recall: 0.5857, F1-score: 0.5863\n",
      "Epoch 19/100 - Accuracy: 0.5905, Precision: 0.5902, Recall: 0.5905, F1-score: 0.5903\n",
      "Epoch 20/100 - Accuracy: 0.5913, Precision: 0.5908, Recall: 0.5913, F1-score: 0.5905\n",
      "Epoch 21/100 - Accuracy: 0.5891, Precision: 0.5905, Recall: 0.5891, F1-score: 0.5896\n",
      "Epoch 22/100 - Accuracy: 0.5929, Precision: 0.5927, Recall: 0.5929, F1-score: 0.5924\n",
      "Epoch 23/100 - Accuracy: 0.5918, Precision: 0.5910, Recall: 0.5918, F1-score: 0.5902\n",
      "Epoch 24/100 - Accuracy: 0.5884, Precision: 0.5892, Recall: 0.5884, F1-score: 0.5843\n",
      "Epoch 25/100 - Accuracy: 0.5916, Precision: 0.5917, Recall: 0.5916, F1-score: 0.5916\n",
      "Epoch 26/100 - Accuracy: 0.5898, Precision: 0.5926, Recall: 0.5898, F1-score: 0.5903\n",
      "Epoch 27/100 - Accuracy: 0.5918, Precision: 0.5908, Recall: 0.5918, F1-score: 0.5904\n",
      "Epoch 28/100 - Accuracy: 0.5916, Precision: 0.5942, Recall: 0.5916, F1-score: 0.5923\n",
      "Epoch 29/100 - Accuracy: 0.5937, Precision: 0.5932, Recall: 0.5937, F1-score: 0.5929\n",
      "Epoch 30/100 - Accuracy: 0.5917, Precision: 0.5907, Recall: 0.5917, F1-score: 0.5900\n",
      "Epoch 31/100 - Accuracy: 0.5926, Precision: 0.5923, Recall: 0.5926, F1-score: 0.5923\n",
      "Epoch 32/100 - Accuracy: 0.5929, Precision: 0.5943, Recall: 0.5929, F1-score: 0.5933\n",
      "Epoch 33/100 - Accuracy: 0.5928, Precision: 0.5918, Recall: 0.5928, F1-score: 0.5919\n",
      "Epoch 34/100 - Accuracy: 0.5937, Precision: 0.5942, Recall: 0.5937, F1-score: 0.5934\n",
      "Epoch 35/100 - Accuracy: 0.5937, Precision: 0.5945, Recall: 0.5937, F1-score: 0.5936\n",
      "Epoch 36/100 - Accuracy: 0.5931, Precision: 0.5961, Recall: 0.5931, F1-score: 0.5938\n",
      "Epoch 37/100 - Accuracy: 0.5926, Precision: 0.5917, Recall: 0.5926, F1-score: 0.5904\n",
      "Epoch 38/100 - Accuracy: 0.5946, Precision: 0.5942, Recall: 0.5946, F1-score: 0.5942\n",
      "Epoch 39/100 - Accuracy: 0.5964, Precision: 0.5957, Recall: 0.5964, F1-score: 0.5952\n",
      "Epoch 40/100 - Accuracy: 0.5917, Precision: 0.5957, Recall: 0.5917, F1-score: 0.5924\n",
      "Epoch 41/100 - Accuracy: 0.5955, Precision: 0.5953, Recall: 0.5955, F1-score: 0.5953\n",
      "Epoch 42/100 - Accuracy: 0.5944, Precision: 0.5966, Recall: 0.5944, F1-score: 0.5950\n",
      "Epoch 43/100 - Accuracy: 0.5940, Precision: 0.5948, Recall: 0.5940, F1-score: 0.5942\n",
      "Epoch 44/100 - Accuracy: 0.5962, Precision: 0.5953, Recall: 0.5962, F1-score: 0.5948\n",
      "Epoch 45/100 - Accuracy: 0.5937, Precision: 0.5963, Recall: 0.5937, F1-score: 0.5944\n",
      "Epoch 46/100 - Accuracy: 0.5929, Precision: 0.5994, Recall: 0.5929, F1-score: 0.5938\n",
      "Epoch 47/100 - Accuracy: 0.5967, Precision: 0.5973, Recall: 0.5967, F1-score: 0.5963\n",
      "Epoch 48/100 - Accuracy: 0.5960, Precision: 0.5981, Recall: 0.5960, F1-score: 0.5958\n",
      "Epoch 49/100 - Accuracy: 0.5972, Precision: 0.5966, Recall: 0.5972, F1-score: 0.5968\n",
      "Epoch 50/100 - Accuracy: 0.5961, Precision: 0.5960, Recall: 0.5961, F1-score: 0.5960\n",
      "Epoch 51/100 - Accuracy: 0.5969, Precision: 0.5972, Recall: 0.5969, F1-score: 0.5954\n",
      "Epoch 52/100 - Accuracy: 0.5969, Precision: 0.5982, Recall: 0.5969, F1-score: 0.5974\n",
      "Epoch 53/100 - Accuracy: 0.5996, Precision: 0.6006, Recall: 0.5996, F1-score: 0.5989\n",
      "Epoch 54/100 - Accuracy: 0.5982, Precision: 0.5998, Recall: 0.5982, F1-score: 0.5983\n",
      "Epoch 55/100 - Accuracy: 0.5989, Precision: 0.5998, Recall: 0.5989, F1-score: 0.5976\n",
      "Epoch 56/100 - Accuracy: 0.5975, Precision: 0.5983, Recall: 0.5975, F1-score: 0.5978\n",
      "Epoch 57/100 - Accuracy: 0.5986, Precision: 0.5996, Recall: 0.5986, F1-score: 0.5985\n",
      "Epoch 58/100 - Accuracy: 0.6000, Precision: 0.6010, Recall: 0.6000, F1-score: 0.5998\n",
      "Epoch 59/100 - Accuracy: 0.5978, Precision: 0.6002, Recall: 0.5978, F1-score: 0.5975\n",
      "Epoch 60/100 - Accuracy: 0.5991, Precision: 0.5984, Recall: 0.5991, F1-score: 0.5981\n",
      "Epoch 61/100 - Accuracy: 0.5988, Precision: 0.5978, Recall: 0.5988, F1-score: 0.5978\n",
      "Epoch 62/100 - Accuracy: 0.5991, Precision: 0.6005, Recall: 0.5991, F1-score: 0.5995\n",
      "Epoch 63/100 - Accuracy: 0.6003, Precision: 0.5999, Recall: 0.6003, F1-score: 0.6000\n",
      "Epoch 64/100 - Accuracy: 0.6013, Precision: 0.6018, Recall: 0.6013, F1-score: 0.6014\n",
      "Epoch 65/100 - Accuracy: 0.6007, Precision: 0.6000, Recall: 0.6007, F1-score: 0.5998\n",
      "Epoch 66/100 - Accuracy: 0.5945, Precision: 0.6005, Recall: 0.5945, F1-score: 0.5942\n",
      "Epoch 67/100 - Accuracy: 0.5983, Precision: 0.6040, Recall: 0.5983, F1-score: 0.5992\n",
      "Epoch 68/100 - Accuracy: 0.6015, Precision: 0.6020, Recall: 0.6015, F1-score: 0.6016\n",
      "Epoch 69/100 - Accuracy: 0.6020, Precision: 0.6014, Recall: 0.6020, F1-score: 0.6016\n",
      "Epoch 70/100 - Accuracy: 0.5999, Precision: 0.6006, Recall: 0.5999, F1-score: 0.6001\n",
      "Epoch 71/100 - Accuracy: 0.6026, Precision: 0.6025, Recall: 0.6026, F1-score: 0.6014\n",
      "Epoch 72/100 - Accuracy: 0.6018, Precision: 0.6032, Recall: 0.6018, F1-score: 0.6019\n",
      "Epoch 73/100 - Accuracy: 0.6015, Precision: 0.6014, Recall: 0.6015, F1-score: 0.6015\n",
      "Epoch 74/100 - Accuracy: 0.6012, Precision: 0.6015, Recall: 0.6012, F1-score: 0.6012\n",
      "Epoch 75/100 - Accuracy: 0.6018, Precision: 0.6041, Recall: 0.6018, F1-score: 0.6025\n",
      "Epoch 76/100 - Accuracy: 0.6019, Precision: 0.6047, Recall: 0.6019, F1-score: 0.6026\n",
      "Epoch 77/100 - Accuracy: 0.6023, Precision: 0.6041, Recall: 0.6023, F1-score: 0.6026\n",
      "Epoch 78/100 - Accuracy: 0.6037, Precision: 0.6068, Recall: 0.6037, F1-score: 0.6044\n",
      "Epoch 79/100 - Accuracy: 0.6009, Precision: 0.6012, Recall: 0.6009, F1-score: 0.5979\n",
      "Epoch 80/100 - Accuracy: 0.6018, Precision: 0.6013, Recall: 0.6018, F1-score: 0.6003\n",
      "Epoch 81/100 - Accuracy: 0.6040, Precision: 0.6055, Recall: 0.6040, F1-score: 0.6045\n",
      "Epoch 82/100 - Accuracy: 0.6035, Precision: 0.6064, Recall: 0.6035, F1-score: 0.6040\n",
      "Epoch 83/100 - Accuracy: 0.6036, Precision: 0.6051, Recall: 0.6036, F1-score: 0.6035\n",
      "Epoch 84/100 - Accuracy: 0.6047, Precision: 0.6056, Recall: 0.6047, F1-score: 0.6048\n",
      "Epoch 85/100 - Accuracy: 0.6045, Precision: 0.6037, Recall: 0.6045, F1-score: 0.6035\n",
      "Epoch 86/100 - Accuracy: 0.6057, Precision: 0.6054, Recall: 0.6057, F1-score: 0.6055\n",
      "Epoch 87/100 - Accuracy: 0.6051, Precision: 0.6054, Recall: 0.6051, F1-score: 0.6043\n",
      "Epoch 88/100 - Accuracy: 0.6040, Precision: 0.6032, Recall: 0.6040, F1-score: 0.6017\n",
      "Epoch 89/100 - Accuracy: 0.6052, Precision: 0.6044, Recall: 0.6052, F1-score: 0.6040\n",
      "Epoch 90/100 - Accuracy: 0.6054, Precision: 0.6056, Recall: 0.6054, F1-score: 0.6048\n",
      "Epoch 91/100 - Accuracy: 0.6063, Precision: 0.6054, Recall: 0.6063, F1-score: 0.6055\n",
      "Epoch 92/100 - Accuracy: 0.6032, Precision: 0.6030, Recall: 0.6032, F1-score: 0.6004\n",
      "Epoch 93/100 - Accuracy: 0.6022, Precision: 0.6037, Recall: 0.6022, F1-score: 0.5998\n",
      "Epoch 94/100 - Accuracy: 0.6077, Precision: 0.6074, Recall: 0.6077, F1-score: 0.6075\n",
      "Epoch 95/100 - Accuracy: 0.6067, Precision: 0.6091, Recall: 0.6067, F1-score: 0.6072\n",
      "Epoch 96/100 - Accuracy: 0.6074, Precision: 0.6092, Recall: 0.6074, F1-score: 0.6079\n",
      "Epoch 97/100 - Accuracy: 0.6089, Precision: 0.6085, Recall: 0.6089, F1-score: 0.6085\n",
      "Epoch 98/100 - Accuracy: 0.6063, Precision: 0.6085, Recall: 0.6063, F1-score: 0.6066\n",
      "Epoch 99/100 - Accuracy: 0.6096, Precision: 0.6101, Recall: 0.6096, F1-score: 0.6096\n",
      "Epoch 100/100 - Accuracy: 0.6099, Precision: 0.6104, Recall: 0.6099, F1-score: 0.6100\n",
      "Training model with logistic activation and w2v vectorizer\n",
      "Epoch 1/100 - Accuracy: 0.3947, Precision: 0.2558, Recall: 0.3947, F1-score: 0.2467\n",
      "Epoch 2/100 - Accuracy: 0.3889, Precision: 0.2564, Recall: 0.3889, F1-score: 0.2688\n",
      "Epoch 3/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 4/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 5/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 6/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 7/100 - Accuracy: 0.3962, Precision: 0.2466, Recall: 0.3962, F1-score: 0.2376\n",
      "Epoch 8/100 - Accuracy: 0.3981, Precision: 0.2448, Recall: 0.3981, F1-score: 0.2329\n",
      "Epoch 9/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 10/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 11/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 12/100 - Accuracy: 0.3991, Precision: 0.2269, Recall: 0.3991, F1-score: 0.2290\n",
      "Epoch 13/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 14/100 - Accuracy: 0.3975, Precision: 0.2606, Recall: 0.3975, F1-score: 0.2498\n",
      "Epoch 15/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 16/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 17/100 - Accuracy: 0.3997, Precision: 0.2226, Recall: 0.3997, F1-score: 0.2285\n",
      "Epoch 18/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 19/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 20/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 21/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 22/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 23/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 24/100 - Accuracy: 0.3831, Precision: 0.2728, Recall: 0.3831, F1-score: 0.3186\n",
      "Epoch 25/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 26/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 27/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 28/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 29/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 30/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 31/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 32/100 - Accuracy: 0.3750, Precision: 0.2734, Recall: 0.3750, F1-score: 0.3137\n",
      "Epoch 33/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 34/100 - Accuracy: 0.3679, Precision: 0.2736, Recall: 0.3679, F1-score: 0.3053\n",
      "Epoch 35/100 - Accuracy: 0.3996, Precision: 0.1598, Recall: 0.3996, F1-score: 0.2283\n",
      "Epoch 36/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 37/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 38/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 39/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 40/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 41/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 42/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 43/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 44/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 45/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 46/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 47/100 - Accuracy: 0.3955, Precision: 0.2744, Recall: 0.3955, F1-score: 0.3166\n",
      "Epoch 48/100 - Accuracy: 0.3997, Precision: 0.1598, Recall: 0.3997, F1-score: 0.2284\n",
      "Epoch 49/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 50/100 - Accuracy: 0.3992, Precision: 0.2443, Recall: 0.3992, F1-score: 0.2311\n",
      "Epoch 51/100 - Accuracy: 0.3918, Precision: 0.2739, Recall: 0.3918, F1-score: 0.3190\n",
      "Epoch 52/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 53/100 - Accuracy: 0.3993, Precision: 0.2445, Recall: 0.3993, F1-score: 0.2306\n",
      "Epoch 54/100 - Accuracy: 0.3996, Precision: 0.2226, Recall: 0.3996, F1-score: 0.2286\n",
      "Epoch 55/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 56/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 57/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 58/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 59/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 60/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 61/100 - Accuracy: 0.3993, Precision: 0.1597, Recall: 0.3993, F1-score: 0.2282\n",
      "Epoch 62/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 63/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 64/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 65/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 66/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 67/100 - Accuracy: 0.3700, Precision: 0.3594, Recall: 0.3700, F1-score: 0.3101\n",
      "Epoch 68/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 69/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 70/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 71/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 72/100 - Accuracy: 0.3992, Precision: 0.2103, Recall: 0.3992, F1-score: 0.2286\n",
      "Epoch 73/100 - Accuracy: 0.3987, Precision: 0.2614, Recall: 0.3987, F1-score: 0.2473\n",
      "Epoch 74/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 75/100 - Accuracy: 0.3712, Precision: 0.2762, Recall: 0.3712, F1-score: 0.3088\n",
      "Epoch 76/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 77/100 - Accuracy: 0.3997, Precision: 0.2226, Recall: 0.3997, F1-score: 0.2285\n",
      "Epoch 78/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 79/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 80/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 81/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 82/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 83/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 84/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 85/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 86/100 - Accuracy: 0.3997, Precision: 0.2295, Recall: 0.3997, F1-score: 0.2286\n",
      "Epoch 87/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 88/100 - Accuracy: 0.3998, Precision: 0.2226, Recall: 0.3998, F1-score: 0.2285\n",
      "Epoch 89/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 90/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 91/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 92/100 - Accuracy: 0.3995, Precision: 0.2336, Recall: 0.3995, F1-score: 0.2289\n",
      "Epoch 93/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 94/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 95/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 96/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 97/100 - Accuracy: 0.4052, Precision: 0.2799, Recall: 0.4052, F1-score: 0.3161\n",
      "Epoch 98/100 - Accuracy: 0.4046, Precision: 0.2769, Recall: 0.4046, F1-score: 0.2797\n",
      "Epoch 99/100 - Accuracy: 0.3994, Precision: 0.2662, Recall: 0.3994, F1-score: 0.2514\n",
      "Epoch 100/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Training model with tanh activation and ft vectorizer\n",
      "Epoch 1/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 2/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 3/100 - Accuracy: 0.3933, Precision: 0.2481, Recall: 0.3933, F1-score: 0.2394\n",
      "Epoch 4/100 - Accuracy: 0.3926, Precision: 0.2587, Recall: 0.3926, F1-score: 0.2522\n",
      "Epoch 5/100 - Accuracy: 0.3898, Precision: 0.2670, Recall: 0.3898, F1-score: 0.2782\n",
      "Epoch 6/100 - Accuracy: 0.3953, Precision: 0.2400, Recall: 0.3953, F1-score: 0.2334\n",
      "Epoch 7/100 - Accuracy: 0.3966, Precision: 0.2392, Recall: 0.3966, F1-score: 0.2319\n",
      "Epoch 8/100 - Accuracy: 0.3914, Precision: 0.2610, Recall: 0.3914, F1-score: 0.2566\n",
      "Epoch 9/100 - Accuracy: 0.3968, Precision: 0.2254, Recall: 0.3968, F1-score: 0.2303\n",
      "Epoch 10/100 - Accuracy: 0.3914, Precision: 0.2576, Recall: 0.3914, F1-score: 0.2533\n",
      "Epoch 11/100 - Accuracy: 0.3883, Precision: 0.2676, Recall: 0.3883, F1-score: 0.2878\n",
      "Epoch 12/100 - Accuracy: 0.3915, Precision: 0.2620, Recall: 0.3915, F1-score: 0.2593\n",
      "Epoch 13/100 - Accuracy: 0.3961, Precision: 0.2389, Recall: 0.3961, F1-score: 0.2322\n",
      "Epoch 14/100 - Accuracy: 0.3890, Precision: 0.2623, Recall: 0.3890, F1-score: 0.2702\n",
      "Epoch 15/100 - Accuracy: 0.3975, Precision: 0.2005, Recall: 0.3975, F1-score: 0.2286\n",
      "Epoch 16/100 - Accuracy: 0.3927, Precision: 0.2584, Recall: 0.3927, F1-score: 0.2509\n",
      "Epoch 17/100 - Accuracy: 0.3974, Precision: 0.2302, Recall: 0.3974, F1-score: 0.2303\n",
      "Epoch 18/100 - Accuracy: 0.3823, Precision: 0.2726, Recall: 0.3823, F1-score: 0.3182\n",
      "Epoch 19/100 - Accuracy: 0.3914, Precision: 0.2611, Recall: 0.3914, F1-score: 0.2565\n",
      "Epoch 20/100 - Accuracy: 0.3938, Precision: 0.2575, Recall: 0.3938, F1-score: 0.2449\n",
      "Epoch 21/100 - Accuracy: 0.3910, Precision: 0.2626, Recall: 0.3910, F1-score: 0.2619\n",
      "Epoch 22/100 - Accuracy: 0.3932, Precision: 0.2583, Recall: 0.3932, F1-score: 0.2483\n",
      "Epoch 23/100 - Accuracy: 0.3928, Precision: 0.2581, Recall: 0.3928, F1-score: 0.2496\n",
      "Epoch 24/100 - Accuracy: 0.3936, Precision: 0.2578, Recall: 0.3936, F1-score: 0.2468\n",
      "Epoch 25/100 - Accuracy: 0.3974, Precision: 0.2073, Recall: 0.3974, F1-score: 0.2290\n",
      "Epoch 26/100 - Accuracy: 0.3938, Precision: 0.2526, Recall: 0.3938, F1-score: 0.2413\n",
      "Epoch 27/100 - Accuracy: 0.3939, Precision: 0.2558, Recall: 0.3939, F1-score: 0.2434\n",
      "Epoch 28/100 - Accuracy: 0.3975, Precision: 0.2279, Recall: 0.3975, F1-score: 0.2300\n",
      "Epoch 29/100 - Accuracy: 0.3914, Precision: 0.2616, Recall: 0.3914, F1-score: 0.2575\n",
      "Epoch 30/100 - Accuracy: 0.3945, Precision: 0.2425, Recall: 0.3945, F1-score: 0.2353\n",
      "Epoch 31/100 - Accuracy: 0.3993, Precision: 0.1598, Recall: 0.3993, F1-score: 0.2282\n",
      "Epoch 32/100 - Accuracy: 0.3946, Precision: 0.2415, Recall: 0.3946, F1-score: 0.2355\n",
      "Epoch 33/100 - Accuracy: 0.3968, Precision: 0.2388, Recall: 0.3968, F1-score: 0.2318\n",
      "Epoch 34/100 - Accuracy: 0.3876, Precision: 0.2729, Recall: 0.3876, F1-score: 0.3162\n",
      "Epoch 35/100 - Accuracy: 0.3903, Precision: 0.2668, Recall: 0.3903, F1-score: 0.2770\n",
      "Epoch 36/100 - Accuracy: 0.3974, Precision: 0.2015, Recall: 0.3974, F1-score: 0.2288\n",
      "Epoch 37/100 - Accuracy: 0.3937, Precision: 0.2521, Recall: 0.3937, F1-score: 0.2415\n",
      "Epoch 38/100 - Accuracy: 0.3914, Precision: 0.2619, Recall: 0.3914, F1-score: 0.2583\n",
      "Epoch 39/100 - Accuracy: 0.3974, Precision: 0.2049, Recall: 0.3974, F1-score: 0.2289\n",
      "Epoch 40/100 - Accuracy: 0.3939, Precision: 0.2461, Recall: 0.3939, F1-score: 0.2378\n",
      "Epoch 41/100 - Accuracy: 0.3880, Precision: 0.2712, Recall: 0.3880, F1-score: 0.3083\n",
      "Epoch 42/100 - Accuracy: 0.3863, Precision: 0.2710, Recall: 0.3863, F1-score: 0.3117\n",
      "Epoch 43/100 - Accuracy: 0.3892, Precision: 0.2661, Recall: 0.3892, F1-score: 0.2792\n",
      "Epoch 44/100 - Accuracy: 0.3936, Precision: 0.2492, Recall: 0.3936, F1-score: 0.2396\n",
      "Epoch 45/100 - Accuracy: 0.3939, Precision: 0.2549, Recall: 0.3939, F1-score: 0.2428\n",
      "Epoch 46/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 47/100 - Accuracy: 0.3902, Precision: 0.2668, Recall: 0.3902, F1-score: 0.2770\n",
      "Epoch 48/100 - Accuracy: 0.3963, Precision: 0.2413, Recall: 0.3963, F1-score: 0.2326\n",
      "Epoch 49/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 50/100 - Accuracy: 0.3895, Precision: 0.2613, Recall: 0.3895, F1-score: 0.2673\n",
      "Epoch 51/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 52/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 53/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 54/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 55/100 - Accuracy: 0.3898, Precision: 0.2618, Recall: 0.3898, F1-score: 0.2641\n",
      "Epoch 56/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 57/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 58/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 59/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 60/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 61/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 62/100 - Accuracy: 0.3891, Precision: 0.2681, Recall: 0.3891, F1-score: 0.2873\n",
      "Epoch 63/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 64/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 65/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 66/100 - Accuracy: 0.3913, Precision: 0.2711, Recall: 0.3913, F1-score: 0.2929\n",
      "Epoch 67/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 68/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 69/100 - Accuracy: 0.3940, Precision: 0.2597, Recall: 0.3940, F1-score: 0.2465\n",
      "Epoch 70/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 71/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 72/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 73/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 74/100 - Accuracy: 0.3904, Precision: 0.2754, Recall: 0.3904, F1-score: 0.3203\n",
      "Epoch 75/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 76/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 77/100 - Accuracy: 0.3909, Precision: 0.2707, Recall: 0.3909, F1-score: 0.2954\n",
      "Epoch 78/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 79/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 80/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 81/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 82/100 - Accuracy: 0.3852, Precision: 0.2729, Recall: 0.3852, F1-score: 0.3185\n",
      "Epoch 83/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 84/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 85/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 86/100 - Accuracy: 0.3825, Precision: 0.2788, Recall: 0.3825, F1-score: 0.3182\n",
      "Epoch 87/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 88/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 89/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 90/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 91/100 - Accuracy: 0.3985, Precision: 0.2595, Recall: 0.3985, F1-score: 0.2384\n",
      "Epoch 92/100 - Accuracy: 0.3993, Precision: 0.2358, Recall: 0.3993, F1-score: 0.2293\n",
      "Epoch 93/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 94/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 95/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 96/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 97/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 98/100 - Accuracy: 0.3993, Precision: 0.2788, Recall: 0.3993, F1-score: 0.3123\n",
      "Epoch 99/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 100/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Training model with tanh activation and gl vectorizer\n",
      "Epoch 1/100 - Accuracy: 0.5594, Precision: 0.5571, Recall: 0.5594, F1-score: 0.5542\n",
      "Epoch 2/100 - Accuracy: 0.5803, Precision: 0.5841, Recall: 0.5803, F1-score: 0.5760\n",
      "Epoch 3/100 - Accuracy: 0.5891, Precision: 0.5902, Recall: 0.5891, F1-score: 0.5884\n",
      "Epoch 4/100 - Accuracy: 0.5953, Precision: 0.5952, Recall: 0.5953, F1-score: 0.5936\n",
      "Epoch 5/100 - Accuracy: 0.5954, Precision: 0.5979, Recall: 0.5954, F1-score: 0.5960\n",
      "Epoch 6/100 - Accuracy: 0.5993, Precision: 0.5987, Recall: 0.5993, F1-score: 0.5988\n",
      "Epoch 7/100 - Accuracy: 0.5994, Precision: 0.5986, Recall: 0.5994, F1-score: 0.5971\n",
      "Epoch 8/100 - Accuracy: 0.6006, Precision: 0.6029, Recall: 0.6006, F1-score: 0.6011\n",
      "Epoch 9/100 - Accuracy: 0.6020, Precision: 0.6013, Recall: 0.6020, F1-score: 0.6015\n",
      "Epoch 10/100 - Accuracy: 0.5999, Precision: 0.6006, Recall: 0.5999, F1-score: 0.5970\n",
      "Epoch 11/100 - Accuracy: 0.6056, Precision: 0.6051, Recall: 0.6056, F1-score: 0.6053\n",
      "Epoch 12/100 - Accuracy: 0.6018, Precision: 0.6021, Recall: 0.6018, F1-score: 0.5983\n",
      "Epoch 13/100 - Accuracy: 0.6052, Precision: 0.6053, Recall: 0.6052, F1-score: 0.6047\n",
      "Epoch 14/100 - Accuracy: 0.6021, Precision: 0.6053, Recall: 0.6021, F1-score: 0.6025\n",
      "Epoch 15/100 - Accuracy: 0.6062, Precision: 0.6084, Recall: 0.6062, F1-score: 0.6067\n",
      "Epoch 16/100 - Accuracy: 0.6085, Precision: 0.6079, Recall: 0.6085, F1-score: 0.6079\n",
      "Epoch 17/100 - Accuracy: 0.6066, Precision: 0.6062, Recall: 0.6066, F1-score: 0.6044\n",
      "Epoch 18/100 - Accuracy: 0.6079, Precision: 0.6077, Recall: 0.6079, F1-score: 0.6078\n",
      "Epoch 19/100 - Accuracy: 0.6124, Precision: 0.6118, Recall: 0.6124, F1-score: 0.6110\n",
      "Epoch 20/100 - Accuracy: 0.6115, Precision: 0.6107, Recall: 0.6115, F1-score: 0.6106\n",
      "Epoch 21/100 - Accuracy: 0.6117, Precision: 0.6127, Recall: 0.6117, F1-score: 0.6115\n",
      "Epoch 22/100 - Accuracy: 0.6114, Precision: 0.6116, Recall: 0.6114, F1-score: 0.6101\n",
      "Epoch 23/100 - Accuracy: 0.6111, Precision: 0.6147, Recall: 0.6111, F1-score: 0.6114\n",
      "Epoch 24/100 - Accuracy: 0.6102, Precision: 0.6113, Recall: 0.6102, F1-score: 0.6089\n",
      "Epoch 25/100 - Accuracy: 0.6130, Precision: 0.6145, Recall: 0.6130, F1-score: 0.6131\n",
      "Epoch 26/100 - Accuracy: 0.6181, Precision: 0.6183, Recall: 0.6181, F1-score: 0.6154\n",
      "Epoch 27/100 - Accuracy: 0.6143, Precision: 0.6175, Recall: 0.6143, F1-score: 0.6150\n",
      "Epoch 28/100 - Accuracy: 0.6164, Precision: 0.6168, Recall: 0.6164, F1-score: 0.6165\n",
      "Epoch 29/100 - Accuracy: 0.6184, Precision: 0.6185, Recall: 0.6184, F1-score: 0.6183\n",
      "Epoch 30/100 - Accuracy: 0.6183, Precision: 0.6192, Recall: 0.6183, F1-score: 0.6186\n",
      "Epoch 31/100 - Accuracy: 0.6212, Precision: 0.6212, Recall: 0.6212, F1-score: 0.6207\n",
      "Epoch 32/100 - Accuracy: 0.6199, Precision: 0.6199, Recall: 0.6199, F1-score: 0.6199\n",
      "Epoch 33/100 - Accuracy: 0.6240, Precision: 0.6233, Recall: 0.6240, F1-score: 0.6230\n",
      "Epoch 34/100 - Accuracy: 0.6242, Precision: 0.6239, Recall: 0.6242, F1-score: 0.6225\n",
      "Epoch 35/100 - Accuracy: 0.6230, Precision: 0.6245, Recall: 0.6230, F1-score: 0.6206\n",
      "Epoch 36/100 - Accuracy: 0.6207, Precision: 0.6233, Recall: 0.6207, F1-score: 0.6211\n",
      "Epoch 37/100 - Accuracy: 0.6272, Precision: 0.6268, Recall: 0.6272, F1-score: 0.6261\n",
      "Epoch 38/100 - Accuracy: 0.6281, Precision: 0.6298, Recall: 0.6281, F1-score: 0.6252\n",
      "Epoch 39/100 - Accuracy: 0.6269, Precision: 0.6261, Recall: 0.6269, F1-score: 0.6261\n",
      "Epoch 40/100 - Accuracy: 0.6243, Precision: 0.6257, Recall: 0.6243, F1-score: 0.6247\n",
      "Epoch 41/100 - Accuracy: 0.6237, Precision: 0.6271, Recall: 0.6237, F1-score: 0.6240\n",
      "Epoch 42/100 - Accuracy: 0.6307, Precision: 0.6314, Recall: 0.6307, F1-score: 0.6304\n",
      "Epoch 43/100 - Accuracy: 0.6321, Precision: 0.6317, Recall: 0.6321, F1-score: 0.6315\n",
      "Epoch 44/100 - Accuracy: 0.6298, Precision: 0.6316, Recall: 0.6298, F1-score: 0.6293\n",
      "Epoch 45/100 - Accuracy: 0.6310, Precision: 0.6329, Recall: 0.6310, F1-score: 0.6311\n",
      "Epoch 46/100 - Accuracy: 0.6344, Precision: 0.6338, Recall: 0.6344, F1-score: 0.6335\n",
      "Epoch 47/100 - Accuracy: 0.6331, Precision: 0.6330, Recall: 0.6331, F1-score: 0.6326\n",
      "Epoch 48/100 - Accuracy: 0.6347, Precision: 0.6350, Recall: 0.6347, F1-score: 0.6322\n",
      "Epoch 49/100 - Accuracy: 0.6360, Precision: 0.6361, Recall: 0.6360, F1-score: 0.6340\n",
      "Epoch 50/100 - Accuracy: 0.6345, Precision: 0.6341, Recall: 0.6345, F1-score: 0.6332\n",
      "Epoch 51/100 - Accuracy: 0.6359, Precision: 0.6361, Recall: 0.6359, F1-score: 0.6337\n",
      "Epoch 52/100 - Accuracy: 0.6342, Precision: 0.6350, Recall: 0.6342, F1-score: 0.6319\n",
      "Epoch 53/100 - Accuracy: 0.6390, Precision: 0.6387, Recall: 0.6390, F1-score: 0.6375\n",
      "Epoch 54/100 - Accuracy: 0.6374, Precision: 0.6385, Recall: 0.6374, F1-score: 0.6365\n",
      "Epoch 55/100 - Accuracy: 0.6389, Precision: 0.6384, Recall: 0.6389, F1-score: 0.6383\n",
      "Epoch 56/100 - Accuracy: 0.6417, Precision: 0.6416, Recall: 0.6417, F1-score: 0.6407\n",
      "Epoch 57/100 - Accuracy: 0.6391, Precision: 0.6399, Recall: 0.6391, F1-score: 0.6392\n",
      "Epoch 58/100 - Accuracy: 0.6412, Precision: 0.6408, Recall: 0.6412, F1-score: 0.6402\n",
      "Epoch 59/100 - Accuracy: 0.6425, Precision: 0.6425, Recall: 0.6425, F1-score: 0.6421\n",
      "Epoch 60/100 - Accuracy: 0.6423, Precision: 0.6425, Recall: 0.6423, F1-score: 0.6409\n",
      "Epoch 61/100 - Accuracy: 0.6400, Precision: 0.6433, Recall: 0.6400, F1-score: 0.6395\n",
      "Epoch 62/100 - Accuracy: 0.6434, Precision: 0.6440, Recall: 0.6434, F1-score: 0.6435\n",
      "Epoch 63/100 - Accuracy: 0.6459, Precision: 0.6462, Recall: 0.6459, F1-score: 0.6443\n",
      "Epoch 64/100 - Accuracy: 0.6471, Precision: 0.6469, Recall: 0.6471, F1-score: 0.6454\n",
      "Epoch 65/100 - Accuracy: 0.6425, Precision: 0.6450, Recall: 0.6425, F1-score: 0.6430\n",
      "Epoch 66/100 - Accuracy: 0.6488, Precision: 0.6485, Recall: 0.6488, F1-score: 0.6475\n",
      "Epoch 67/100 - Accuracy: 0.6485, Precision: 0.6487, Recall: 0.6485, F1-score: 0.6486\n",
      "Epoch 68/100 - Accuracy: 0.6438, Precision: 0.6466, Recall: 0.6438, F1-score: 0.6444\n",
      "Epoch 69/100 - Accuracy: 0.6482, Precision: 0.6490, Recall: 0.6482, F1-score: 0.6457\n",
      "Epoch 70/100 - Accuracy: 0.6477, Precision: 0.6473, Recall: 0.6477, F1-score: 0.6473\n",
      "Epoch 71/100 - Accuracy: 0.6495, Precision: 0.6494, Recall: 0.6495, F1-score: 0.6486\n",
      "Epoch 72/100 - Accuracy: 0.6501, Precision: 0.6511, Recall: 0.6501, F1-score: 0.6504\n",
      "Epoch 73/100 - Accuracy: 0.6484, Precision: 0.6498, Recall: 0.6484, F1-score: 0.6487\n",
      "Epoch 74/100 - Accuracy: 0.6477, Precision: 0.6481, Recall: 0.6477, F1-score: 0.6469\n",
      "Epoch 75/100 - Accuracy: 0.6528, Precision: 0.6530, Recall: 0.6528, F1-score: 0.6527\n",
      "Epoch 76/100 - Accuracy: 0.6548, Precision: 0.6541, Recall: 0.6548, F1-score: 0.6541\n",
      "Epoch 77/100 - Accuracy: 0.6539, Precision: 0.6537, Recall: 0.6539, F1-score: 0.6533\n",
      "Epoch 78/100 - Accuracy: 0.6512, Precision: 0.6544, Recall: 0.6512, F1-score: 0.6519\n",
      "Epoch 79/100 - Accuracy: 0.6541, Precision: 0.6535, Recall: 0.6541, F1-score: 0.6532\n",
      "Epoch 80/100 - Accuracy: 0.6537, Precision: 0.6559, Recall: 0.6537, F1-score: 0.6540\n",
      "Epoch 81/100 - Accuracy: 0.6544, Precision: 0.6576, Recall: 0.6544, F1-score: 0.6546\n",
      "Epoch 82/100 - Accuracy: 0.6569, Precision: 0.6581, Recall: 0.6569, F1-score: 0.6573\n",
      "Epoch 83/100 - Accuracy: 0.6590, Precision: 0.6592, Recall: 0.6590, F1-score: 0.6573\n",
      "Epoch 84/100 - Accuracy: 0.6589, Precision: 0.6589, Recall: 0.6589, F1-score: 0.6583\n",
      "Epoch 85/100 - Accuracy: 0.6593, Precision: 0.6589, Recall: 0.6593, F1-score: 0.6581\n",
      "Epoch 86/100 - Accuracy: 0.6561, Precision: 0.6561, Recall: 0.6561, F1-score: 0.6544\n",
      "Epoch 87/100 - Accuracy: 0.6611, Precision: 0.6607, Recall: 0.6611, F1-score: 0.6600\n",
      "Epoch 88/100 - Accuracy: 0.6604, Precision: 0.6600, Recall: 0.6604, F1-score: 0.6593\n",
      "Epoch 89/100 - Accuracy: 0.6631, Precision: 0.6627, Recall: 0.6631, F1-score: 0.6622\n",
      "Epoch 90/100 - Accuracy: 0.6618, Precision: 0.6621, Recall: 0.6618, F1-score: 0.6619\n",
      "Epoch 91/100 - Accuracy: 0.6605, Precision: 0.6627, Recall: 0.6605, F1-score: 0.6577\n",
      "Epoch 92/100 - Accuracy: 0.6631, Precision: 0.6639, Recall: 0.6631, F1-score: 0.6626\n",
      "Epoch 93/100 - Accuracy: 0.6627, Precision: 0.6632, Recall: 0.6627, F1-score: 0.6623\n",
      "Epoch 94/100 - Accuracy: 0.6647, Precision: 0.6659, Recall: 0.6647, F1-score: 0.6651\n",
      "Epoch 95/100 - Accuracy: 0.6663, Precision: 0.6660, Recall: 0.6663, F1-score: 0.6657\n",
      "Epoch 96/100 - Accuracy: 0.6570, Precision: 0.6656, Recall: 0.6570, F1-score: 0.6512\n",
      "Epoch 97/100 - Accuracy: 0.6681, Precision: 0.6688, Recall: 0.6681, F1-score: 0.6681\n",
      "Epoch 98/100 - Accuracy: 0.6645, Precision: 0.6664, Recall: 0.6645, F1-score: 0.6618\n",
      "Epoch 99/100 - Accuracy: 0.6668, Precision: 0.6667, Recall: 0.6668, F1-score: 0.6653\n",
      "Epoch 100/100 - Accuracy: 0.6637, Precision: 0.6699, Recall: 0.6637, F1-score: 0.6645\n",
      "Training model with tanh activation and w2v vectorizer\n",
      "Epoch 1/100 - Accuracy: 0.3696, Precision: 0.2688, Recall: 0.3696, F1-score: 0.3096\n",
      "Epoch 2/100 - Accuracy: 0.3511, Precision: 0.3968, Recall: 0.3511, F1-score: 0.3238\n",
      "Epoch 3/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 4/100 - Accuracy: 0.3982, Precision: 0.2128, Recall: 0.3982, F1-score: 0.2300\n",
      "Epoch 5/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 6/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 7/100 - Accuracy: 0.3958, Precision: 0.3419, Recall: 0.3958, F1-score: 0.2408\n",
      "Epoch 8/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 9/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 10/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 11/100 - Accuracy: 0.3669, Precision: 0.2593, Recall: 0.3669, F1-score: 0.3011\n",
      "Epoch 12/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 13/100 - Accuracy: 0.3604, Precision: 0.3792, Recall: 0.3604, F1-score: 0.3433\n",
      "Epoch 14/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 15/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 16/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 17/100 - Accuracy: 0.3996, Precision: 0.1598, Recall: 0.3996, F1-score: 0.2283\n",
      "Epoch 18/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 19/100 - Accuracy: 0.3991, Precision: 0.3011, Recall: 0.3991, F1-score: 0.2332\n",
      "Epoch 20/100 - Accuracy: 0.3995, Precision: 0.2225, Recall: 0.3995, F1-score: 0.2286\n",
      "Epoch 21/100 - Accuracy: 0.3723, Precision: 0.3785, Recall: 0.3723, F1-score: 0.3237\n",
      "Epoch 22/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 23/100 - Accuracy: 0.3678, Precision: 0.3817, Recall: 0.3678, F1-score: 0.3082\n",
      "Epoch 24/100 - Accuracy: 0.3652, Precision: 0.3798, Recall: 0.3652, F1-score: 0.3657\n",
      "Epoch 25/100 - Accuracy: 0.3455, Precision: 0.2910, Recall: 0.3455, F1-score: 0.2488\n",
      "Epoch 26/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 27/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 28/100 - Accuracy: 0.3990, Precision: 0.2293, Recall: 0.3990, F1-score: 0.2304\n",
      "Epoch 29/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 30/100 - Accuracy: 0.3996, Precision: 0.1598, Recall: 0.3996, F1-score: 0.2283\n",
      "Epoch 31/100 - Accuracy: 0.3625, Precision: 0.3828, Recall: 0.3625, F1-score: 0.3060\n",
      "Epoch 32/100 - Accuracy: 0.3994, Precision: 0.2552, Recall: 0.3994, F1-score: 0.2292\n",
      "Epoch 33/100 - Accuracy: 0.3945, Precision: 0.4459, Recall: 0.3945, F1-score: 0.3036\n",
      "Epoch 34/100 - Accuracy: 0.3995, Precision: 0.1598, Recall: 0.3995, F1-score: 0.2283\n",
      "Epoch 35/100 - Accuracy: 0.3605, Precision: 0.5757, Recall: 0.3605, F1-score: 0.2956\n",
      "Epoch 36/100 - Accuracy: 0.3723, Precision: 0.3788, Recall: 0.3723, F1-score: 0.3471\n",
      "Epoch 37/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 38/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 39/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 40/100 - Accuracy: 0.3669, Precision: 0.3839, Recall: 0.3669, F1-score: 0.3669\n",
      "Epoch 41/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 42/100 - Accuracy: 0.3706, Precision: 0.3869, Recall: 0.3706, F1-score: 0.3675\n",
      "Epoch 43/100 - Accuracy: 0.3998, Precision: 0.2890, Recall: 0.3998, F1-score: 0.2293\n",
      "Epoch 44/100 - Accuracy: 0.3999, Precision: 0.2740, Recall: 0.3999, F1-score: 0.2290\n",
      "Epoch 45/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 46/100 - Accuracy: 0.3690, Precision: 0.2751, Recall: 0.3690, F1-score: 0.3061\n",
      "Epoch 47/100 - Accuracy: 0.3978, Precision: 0.2618, Recall: 0.3978, F1-score: 0.2985\n",
      "Epoch 48/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 49/100 - Accuracy: 0.4005, Precision: 0.2792, Recall: 0.4005, F1-score: 0.2480\n",
      "Epoch 50/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 51/100 - Accuracy: 0.3997, Precision: 0.1598, Recall: 0.3997, F1-score: 0.2284\n",
      "Epoch 52/100 - Accuracy: 0.4047, Precision: 0.3898, Recall: 0.4047, F1-score: 0.3350\n",
      "Epoch 53/100 - Accuracy: 0.3621, Precision: 0.3856, Recall: 0.3621, F1-score: 0.3587\n",
      "Epoch 54/100 - Accuracy: 0.3914, Precision: 0.2771, Recall: 0.3914, F1-score: 0.3240\n",
      "Epoch 55/100 - Accuracy: 0.4045, Precision: 0.3924, Recall: 0.4045, F1-score: 0.2780\n",
      "Epoch 56/100 - Accuracy: 0.3885, Precision: 0.3909, Recall: 0.3885, F1-score: 0.3380\n",
      "Epoch 57/100 - Accuracy: 0.3999, Precision: 0.3167, Recall: 0.3999, F1-score: 0.2286\n",
      "Epoch 58/100 - Accuracy: 0.3849, Precision: 0.2635, Recall: 0.3849, F1-score: 0.3128\n",
      "Epoch 59/100 - Accuracy: 0.3999, Precision: 0.4735, Recall: 0.3999, F1-score: 0.2286\n",
      "Epoch 60/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 61/100 - Accuracy: 0.3998, Precision: 0.3167, Recall: 0.3998, F1-score: 0.2285\n",
      "Epoch 62/100 - Accuracy: 0.3997, Precision: 0.1598, Recall: 0.3997, F1-score: 0.2284\n",
      "Epoch 63/100 - Accuracy: 0.3999, Precision: 0.4735, Recall: 0.3999, F1-score: 0.2286\n",
      "Epoch 64/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 65/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 66/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 67/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 68/100 - Accuracy: 0.3996, Precision: 0.1598, Recall: 0.3996, F1-score: 0.2283\n",
      "Epoch 69/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 70/100 - Accuracy: 0.3999, Precision: 0.4735, Recall: 0.3999, F1-score: 0.2286\n",
      "Epoch 71/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 72/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 73/100 - Accuracy: 0.4001, Precision: 0.3776, Recall: 0.4001, F1-score: 0.2307\n",
      "Epoch 74/100 - Accuracy: 0.3997, Precision: 0.2805, Recall: 0.3997, F1-score: 0.2297\n",
      "Epoch 75/100 - Accuracy: 0.3996, Precision: 0.1598, Recall: 0.3996, F1-score: 0.2283\n",
      "Epoch 76/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 77/100 - Accuracy: 0.4006, Precision: 0.2634, Recall: 0.4006, F1-score: 0.2853\n",
      "Epoch 78/100 - Accuracy: 0.3996, Precision: 0.1598, Recall: 0.3996, F1-score: 0.2283\n",
      "Epoch 79/100 - Accuracy: 0.4001, Precision: 0.2819, Recall: 0.4001, F1-score: 0.2295\n",
      "Epoch 80/100 - Accuracy: 0.3931, Precision: 0.2623, Recall: 0.3931, F1-score: 0.3087\n",
      "Epoch 81/100 - Accuracy: 0.4003, Precision: 0.2849, Recall: 0.4003, F1-score: 0.2345\n",
      "Epoch 82/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 83/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 84/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 85/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 86/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 87/100 - Accuracy: 0.3996, Precision: 0.2008, Recall: 0.3996, F1-score: 0.2285\n",
      "Epoch 88/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 89/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 90/100 - Accuracy: 0.3996, Precision: 0.5583, Recall: 0.3996, F1-score: 0.2296\n",
      "Epoch 91/100 - Accuracy: 0.4127, Precision: 0.2959, Recall: 0.4127, F1-score: 0.2963\n",
      "Epoch 92/100 - Accuracy: 0.3995, Precision: 0.1598, Recall: 0.3995, F1-score: 0.2283\n",
      "Epoch 93/100 - Accuracy: 0.4132, Precision: 0.2954, Recall: 0.4132, F1-score: 0.2988\n",
      "Epoch 94/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n",
      "Epoch 95/100 - Accuracy: 0.4006, Precision: 0.2985, Recall: 0.4006, F1-score: 0.2332\n",
      "Epoch 96/100 - Accuracy: 0.3887, Precision: 0.3976, Recall: 0.3887, F1-score: 0.3162\n",
      "Epoch 97/100 - Accuracy: 0.4055, Precision: 0.3736, Recall: 0.4055, F1-score: 0.2705\n",
      "Epoch 98/100 - Accuracy: 0.4025, Precision: 0.3722, Recall: 0.4025, F1-score: 0.2735\n",
      "Epoch 99/100 - Accuracy: 0.3992, Precision: 0.2251, Recall: 0.3992, F1-score: 0.2289\n",
      "Epoch 100/100 - Accuracy: 0.3998, Precision: 0.1599, Recall: 0.3998, F1-score: 0.2284\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    MLPClassifier(hidden_layer_sizes=(32, 64), max_iter=1, activation='logistic'),\n",
    "    MLPClassifier(hidden_layer_sizes=(32, 64), max_iter=1, activation='tanh')\n",
    "]\n",
    "\n",
    "vectorizers = ['ft', 'gl', 'w2v']  # Add w2v\n",
    "losses = {}\n",
    "\n",
    "for model in models:\n",
    "    for vectorizer in vectorizers:\n",
    "        print(f\"Training model with {model.activation} activation and {vectorizer} vectorizer\")\n",
    "        loss = train(vectorizer, model, 100)\n",
    "        losses[f\"{model.activation}_{vectorizer}\"] = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40dec5a4-ad88-4578-a3da-69280adeb352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with logistic activation and w2v vectorizer\n",
      "Training model with logistic activation and ft vectorizer\n",
      "Training model with logistic activation and gl vectorizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with tanh activation and w2v vectorizer\n",
      "Training model with tanh activation and ft vectorizer\n",
      "Training model with tanh activation and gl vectorizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hafsa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "def train_and_evaluate(vectorizer, model):\n",
    "    if vectorizer == 'w2v':\n",
    "        X_train_vec, X_test_vec = X_train_w2v, X_test_w2v\n",
    "    elif vectorizer == 'ft':\n",
    "        X_train_vec, X_test_vec = X_train_ft, X_test_ft\n",
    "    elif vectorizer == 'gl':\n",
    "        X_train_vec, X_test_vec = X_train_glove, X_test_glove\n",
    "    else:\n",
    "        raise ValueError(\"Invalid vectorizer choice!\")\n",
    "\n",
    "    model.fit(X_train_vec, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test_vec)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "models = [\n",
    "    MLPClassifier(hidden_layer_sizes=(32, 64), max_iter=100, activation='logistic'),\n",
    "    MLPClassifier(hidden_layer_sizes=(32, 64), max_iter=100, activation='tanh')\n",
    "]\n",
    "\n",
    "vectorizers = ['w2v', 'ft', 'gl'] \n",
    "evaluation_results = []\n",
    "\n",
    "for model in models:\n",
    "    for vectorizer in vectorizers:\n",
    "        print(f\"Training model with {model.activation} activation and {vectorizer} vectorizer\")\n",
    "        \n",
    "        accuracy, precision, recall, f1 = train_and_evaluate(vectorizer, model)\n",
    "\n",
    "        evaluation_results.append({\n",
    "            'Model': model.activation,\n",
    "            'Vectorizer': vectorizer,\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-score': f1\n",
    "        })\n",
    "\n",
    "evaluation = pd.DataFrame(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3abfe2d3-553f-4b7d-bc5d-a4f5f5151002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>logistic</td>\n",
       "      <td>w2v</td>\n",
       "      <td>0.411985</td>\n",
       "      <td>0.169732</td>\n",
       "      <td>0.411985</td>\n",
       "      <td>0.240416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>logistic</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.257566</td>\n",
       "      <td>0.394961</td>\n",
       "      <td>0.280625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistic</td>\n",
       "      <td>gl</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.595056</td>\n",
       "      <td>0.595506</td>\n",
       "      <td>0.595247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tanh</td>\n",
       "      <td>w2v</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.270071</td>\n",
       "      <td>0.352400</td>\n",
       "      <td>0.291409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tanh</td>\n",
       "      <td>ft</td>\n",
       "      <td>0.411645</td>\n",
       "      <td>0.275096</td>\n",
       "      <td>0.411645</td>\n",
       "      <td>0.247362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tanh</td>\n",
       "      <td>gl</td>\n",
       "      <td>0.614573</td>\n",
       "      <td>0.614209</td>\n",
       "      <td>0.614573</td>\n",
       "      <td>0.614334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model Vectorizer  Accuracy  Precision    Recall  F1-score\n",
       "0  logistic        w2v  0.411985   0.169732  0.411985  0.240416\n",
       "1  logistic         ft  0.394961   0.257566  0.394961  0.280625\n",
       "2  logistic         gl  0.595506   0.595056  0.595506  0.595247\n",
       "3      tanh        w2v  0.352400   0.270071  0.352400  0.291409\n",
       "4      tanh         ft  0.411645   0.275096  0.411645  0.247362\n",
       "5      tanh         gl  0.614573   0.614209  0.614573  0.614334"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48ae46-9f30-406c-9221-97a8f13bb518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
